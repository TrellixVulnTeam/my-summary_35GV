{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 1\n",
    "\n",
    "# Ранжирование вопросов StackOverflow с помощью векторных представлений слов\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: Власов Андрей Валерьевич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "В этом задании вы научитесь вычислять близость текстов и применить этот метод для поиска похожих вопросов на [StackOverflow](https://stackoverflow.com).\n",
    "\n",
    "### Используемые библиотеки\n",
    "\n",
    "В данном задании потребуются следующие библиотеки:\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — инструмент для решения различных задач NLP (тематическое моделирование, представление текстов, ...).\n",
    "- [Numpy](http://www.numpy.org) — библиотека для научных вычислений.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — библилиотека с многими реализованными алгоритмами машинного обучения для анализа данных.\n",
    "- [Nltk](http://www.nltk.org) — инструмент для работы с естественными языками.\n",
    "\n",
    "Для выполнения бонусной части потребуется:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — универсальная модель для обучения различных векторных представлений, разработанная командой Facebook.\n",
    "\n",
    "\n",
    "### Данные\n",
    "\n",
    "Данные лежат в архиве `StackOverflowData.zip`, который состоит из:\n",
    "- `train.tsv` - обучающая выборка. В каждой строке через табуляцию записаны дублирующие друг друга предложения;\n",
    "- `test.tsv` - тестовая выборка. В каждой строке через табуляцию записаны: *<вопрос>, <похожий вопрос>, <отрицательный пример 1>, <отрицательный пример 2>, ...*\n",
    "\n",
    "Скачать архив можно здесь: [ссылка на google диск](https://drive.google.com/open?id=1QqT4D0EoqJTy7v9VrNCYD-m964XZFR7_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вектора слов/\n",
    "\n",
    "Для решения вам потребуются две модели векторных представлений слов:\n",
    "\n",
    " - [Предобученные векторные представления слов](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit), которые были обучены с помощью стандартной модели word2vec на данных Google News (100 миллиардов слов). Модель содержит 300-мерные вектора для 3 миллионов слов и фраз. Вы можете скачать их, запустив блок кода ниже.\n",
    " - Векторные представления слов, полученные с помощью StarSpace на выборке StackOverflow. Вам потребуется обучить эту модель самим во второй части задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Google vectors to directory *target_dir*\n",
    "\n",
    "from download_utils import download_google_vectors\n",
    "download_google_vectors(target_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Предобученные векторные представления слов (2 балла)\n",
    "\n",
    "Скачайте предобученные вектора и загрузите их с помощью функции [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) библиотеки Gensim с параметром *binary=True*. Если суммарный размер векторов больше, чем доступная память, то вы можете загрузите только часть векторов, указав параметр *limit* (рекомендуемое значение: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', \n",
    "                                                             binary=True, limit=500000)\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как пользоваться этими векторами?\n",
    "\n",
    "Как только вы загрузите векторные представления слов в память, убедитесь, что имеете к ним доступ. Сначала вы можете проверить, содержится ли какое-то слово в загруженных эмбедингах:\n",
    "\n",
    "    'word' in wv_embeddings\n",
    "\n",
    "Затем, чтобы получить соответствующий вектор, вы можете использовать оператор доступа по ключу:\n",
    "\n",
    "    wv_embeddings['word']\n",
    "\n",
    "### Проверим, корректны ли векторные представления\n",
    "\n",
    "Чтобы предотвратить возможные ошибки во время первого этапа, можно проверить, что загруженные вектора корректны. Для этого вы можете запустить функцию *check_embeddings*. Она запускает 3 теста:\n",
    "1. Находит наиболее похожие слова для заданных \"положительных\" и \"отрицательных\" слов.\n",
    "2. Находит, какое слово из заданного списка не встречается с остальными.\n",
    "3. Находит наиболее похожее слово для заданного."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrey/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py:858: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "from tests import check_embeddings\n",
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторные представления текста\n",
    "\n",
    "Чтобы перейти от отдельных слов к векторным представлениям вопросов, предлагается подсчитать **среднее** векторов всех слов в вопросе. Если для какого-то слова нет предобученного вектоора, то его нужно пропустить. Если вопрос не содержит ни одного известного слова, то нужно вернуть нулевой вектор. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Реализуйте функцию *question_to_vec_by_mean*, работающую по такой логике. **</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_by_mean(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    words = question.split(' ')\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "            if word in embeddings:\n",
    "                vectors.append(embeddings[word])\n",
    "    if len(vectors) == 0:\n",
    "        vectors.append(np.zeros(dim))\n",
    "    result = np.mean(vectors, axis=0)\n",
    "    \n",
    "    return result\n",
    "    ##########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ \n",
    "    ##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для базовой проверки решения запустите клетку ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "from tests import question_to_vec_tests\n",
    "print(question_to_vec_tests(question_to_vec_by_mean, wv_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.59375000e-01,  4.15039062e-02,  9.03320312e-02,  5.46875000e-02,\n",
       "       -1.47460938e-01,  4.76074219e-02, -8.49609375e-02, -2.04101562e-01,\n",
       "        3.10546875e-01, -1.05590820e-02, -6.15234375e-02, -1.55273438e-01,\n",
       "       -1.52343750e-01,  8.54492188e-02, -2.70996094e-02,  3.84765625e-01,\n",
       "        4.78515625e-02,  2.58789062e-02,  4.49218750e-02, -2.79296875e-01,\n",
       "        9.09423828e-03,  4.08203125e-01,  2.40234375e-01, -3.06640625e-01,\n",
       "       -1.80664062e-01,  4.73632812e-02, -2.63671875e-01,  9.08203125e-02,\n",
       "        1.37695312e-01, -7.20977783e-04,  2.67333984e-02,  1.92382812e-01,\n",
       "       -2.29492188e-02,  9.70458984e-03, -7.37304688e-02,  4.29687500e-01,\n",
       "       -7.93457031e-03,  1.06445312e-01,  2.80761719e-02, -2.29492188e-01,\n",
       "       -1.91650391e-02, -2.36816406e-02,  3.51562500e-02,  1.71875000e-01,\n",
       "       -1.12304688e-01,  6.25000000e-02, -1.69921875e-01,  1.29882812e-01,\n",
       "       -1.54296875e-01,  1.58203125e-01, -7.76367188e-02,  1.78710938e-01,\n",
       "       -1.72851562e-01,  9.96093750e-02,  3.94531250e-01,  6.44531250e-02,\n",
       "       -6.83593750e-02, -3.18359375e-01,  5.95703125e-02, -1.02539062e-02,\n",
       "        9.37500000e-02,  8.25195312e-02, -2.52685547e-02,  1.09863281e-01,\n",
       "       -6.73828125e-02, -1.70898438e-01,  6.78710938e-02,  1.04492188e-01,\n",
       "       -2.11914062e-01,  1.30859375e-01, -1.24573708e-05,  1.85546875e-02,\n",
       "       -1.61132812e-01, -8.00781250e-02,  9.42382812e-02, -8.78906250e-02,\n",
       "        1.82617188e-01, -2.48718262e-03,  8.74023438e-02,  1.75781250e-01,\n",
       "       -2.17285156e-02, -1.96289062e-01,  9.52148438e-02, -5.15136719e-02,\n",
       "        1.01928711e-02,  6.22558594e-02, -2.13867188e-01,  2.25585938e-01,\n",
       "        2.46093750e-01, -6.12792969e-02,  1.74560547e-02, -1.46484375e-01,\n",
       "        3.93676758e-03, -1.62109375e-01, -1.10839844e-01,  6.88476562e-02,\n",
       "       -1.83593750e-01,  1.13281250e-01,  9.08203125e-02, -1.64062500e-01,\n",
       "       -3.71093750e-01, -5.39550781e-02, -8.66699219e-03, -1.18164062e-01,\n",
       "       -5.93261719e-02,  8.74023438e-02, -1.98242188e-01, -1.36718750e-01,\n",
       "        6.39648438e-02, -1.88476562e-01, -2.96875000e-01,  6.39648438e-02,\n",
       "        2.16796875e-01, -7.71484375e-02,  1.13769531e-01,  1.96533203e-02,\n",
       "        2.31445312e-01,  6.59179688e-02,  1.02539062e-01, -6.39648438e-02,\n",
       "       -1.48437500e-01, -5.59082031e-02, -2.43164062e-01,  2.71484375e-01,\n",
       "        1.83593750e-01,  3.06396484e-02, -2.01416016e-02, -1.53320312e-01,\n",
       "        7.08007812e-02, -2.35595703e-02, -9.66796875e-02, -2.83203125e-01,\n",
       "       -2.57568359e-02, -7.42187500e-02, -4.27246094e-02,  6.98242188e-02,\n",
       "       -1.74804688e-01,  2.27539062e-01,  2.92968750e-01, -1.86767578e-02,\n",
       "        2.94921875e-01, -1.12304688e-01,  4.85839844e-02, -2.15820312e-01,\n",
       "        1.03149414e-02, -1.14257812e-01, -6.39648438e-02,  7.27539062e-02,\n",
       "       -1.47460938e-01, -2.16796875e-01,  1.32812500e-01,  1.83593750e-01,\n",
       "       -1.48437500e-01, -1.31835938e-01, -3.73535156e-02,  1.19628906e-01,\n",
       "       -2.01171875e-01,  1.00097656e-01, -8.93554688e-02,  1.23596191e-03,\n",
       "        7.17773438e-02,  1.42578125e-01, -3.03955078e-02, -1.89453125e-01,\n",
       "       -8.88671875e-02,  3.83300781e-02, -1.74804688e-01, -3.66210938e-03,\n",
       "       -2.08007812e-01,  8.97216797e-03,  2.35351562e-01,  1.06933594e-01,\n",
       "       -2.65625000e-01, -2.16796875e-01,  7.08007812e-02,  9.08203125e-02,\n",
       "        3.00781250e-01, -1.07421875e-01,  1.01562500e-01, -6.25000000e-02,\n",
       "        1.33789062e-01, -1.62353516e-02,  2.50000000e-01, -1.72851562e-01,\n",
       "        3.32031250e-01,  1.12304688e-01, -1.47705078e-02, -1.04980469e-01,\n",
       "       -8.05664062e-02,  3.30078125e-01,  9.32617188e-02, -1.47460938e-01,\n",
       "       -2.05078125e-01, -7.56835938e-02, -1.04492188e-01,  6.25000000e-02,\n",
       "       -2.02148438e-01, -1.09375000e-01, -8.05664062e-02,  5.49316406e-02,\n",
       "       -8.88671875e-02,  5.24902344e-02, -2.23632812e-01,  5.17578125e-02,\n",
       "       -1.83593750e-01, -6.73828125e-02, -9.13085938e-02,  1.29882812e-01,\n",
       "       -2.31933594e-02, -1.04003906e-01,  1.79687500e-01,  8.34960938e-02,\n",
       "       -8.78906250e-02, -2.17773438e-01, -6.34765625e-02,  1.33789062e-01,\n",
       "        1.62109375e-01,  2.87109375e-01, -1.14257812e-01,  6.05468750e-02,\n",
       "        1.49414062e-01, -3.08227539e-03,  1.96289062e-01, -8.98437500e-02,\n",
       "        1.45507812e-01,  1.02539062e-02,  1.22070312e-02,  3.20312500e-01,\n",
       "        1.24511719e-01,  1.20849609e-02, -1.78710938e-01,  3.71093750e-02,\n",
       "        6.98242188e-02,  1.62109375e-01,  9.86328125e-02, -2.61718750e-01,\n",
       "        1.89453125e-01, -2.83203125e-02,  4.06250000e-01,  3.56445312e-02,\n",
       "        3.10058594e-02,  2.27050781e-02,  1.30859375e-01, -1.05957031e-01,\n",
       "        8.69140625e-02, -9.76562500e-02,  1.89453125e-01,  3.17382812e-02,\n",
       "        1.10351562e-01,  2.11914062e-01, -1.66992188e-01,  1.45263672e-02,\n",
       "        1.15234375e-01,  1.59179688e-01,  9.91210938e-02, -2.40234375e-01,\n",
       "       -2.34375000e-01,  1.74804688e-01,  1.20605469e-01, -3.67187500e-01,\n",
       "       -7.81250000e-02,  1.10839844e-01, -3.35937500e-01, -9.81445312e-02,\n",
       "       -7.47070312e-02, -1.89453125e-01,  7.81250000e-02, -2.53906250e-01,\n",
       "       -6.03027344e-02, -2.46093750e-01, -9.37500000e-02,  8.64257812e-02,\n",
       "        1.15722656e-01, -1.24511719e-01,  1.61132812e-01, -6.03027344e-02,\n",
       "       -2.47070312e-01, -9.52148438e-02, -4.05273438e-02,  2.51953125e-01,\n",
       "       -1.95312500e-01, -1.31835938e-01,  6.88476562e-02,  2.67333984e-02,\n",
       "        1.03027344e-01,  1.05957031e-01, -3.01513672e-02,  3.04687500e-01,\n",
       "       -8.74023438e-02,  1.19140625e-01, -1.74560547e-02,  8.78906250e-03,\n",
       "       -1.38671875e-01, -2.85156250e-01,  2.29492188e-01, -3.55468750e-01,\n",
       "        9.52148438e-03, -4.07714844e-02, -8.88671875e-02, -1.39160156e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_to_vec_by_mean('word', wv_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть метод для создания векторного представления любого предложения. Оценим, как будет работать это решение.\n",
    "\n",
    "### Оценка близости текстов\n",
    "\n",
    "Представим, что мы используем идеальные векторные представления слов. Тогда косинусное расстояние между дублирующими предложениями должно быть меньше, чем между случайно взятыми предложениями. \n",
    "\n",
    "Сгенерируем для каждого из *N* вопросов *R* случайных отрицательных примеров и примешаем к ним также настоящие дубликаты. Для каждого вопроса будем ранжировать с помощью нашей модели *R + 1* примеров и смотреть на позицию дубликата.\n",
    "\n",
    "#### Hits@K\n",
    "Первой простой метрикой будет количество корректных попаданий для какого-то *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)],$$\n",
    "где $q_i$ - $i$-ый вопрос, $dup_i$ - его дубликат, $topK(q_i)$ - первые *K* элементов в ранжированном списке, который выдает наша модель.\n",
    "\n",
    "#### DCG@K\n",
    "Второй метрикой будет упрощенная [DCG метрика](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K],$$\n",
    "где $rank_{dup_i}$ - позиция дубликата в ранжированном списке ближайших предложений для вопроса $q_i$. С такой метрикой модель штрафуется за низкую позицию корректного ответа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример оценок\n",
    "\n",
    "Вычислим описанные выше метрики для игрушечного примера. Пусть $N = 1$, $R = 3$, вопрос $q_1$ это \"Что такое python\", а его дубликат $dup_1$ это \"Что такое язык python\". Пусть модель выдала следующий ранжированный список кандидатов:\n",
    "\n",
    "1. *\"Как узнать с++\"*\n",
    "2. *\"Что такое язык python\"*\n",
    "3. *\"Хочу учить Java\"*\n",
    "4. *\"Не понимаю Tensorflow\"*\n",
    "\n",
    "Вычислим метрику *Hits@K* для *K = 1, 4*:\n",
    "\n",
    "- [K = 1] $\\text{Hits@1} =  [dup_1 \\in top1(q_1)] = 0$\n",
    "- [K = 4] $\\text{Hits@4} =  [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Вычислим метрику *DCG@K* для *K = 1, 4*:\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{\\log_2(1+2)}\\cdot[2 \\le 1] = 0$\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{\\log_2(1+2)}\\cdot[2 \\le 4] = \\frac{1}{\\log_2{3}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Реализуйте функции *hits_count* и *dcg_score*. **</font> \n",
    "\n",
    "Каждая функция имеет два аргумента: *dup_ranks* и *k*. *dup_ranks* является списком, который содржит *рейтинги дубликатов* (их позиции в ранжированном списке). Например, *dup_ranks = [2]* для примера, описанного выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of ranks of the duplicates; one rank per question; \n",
    "                   length is a number of questions that we check (N); \n",
    "                   rank is a number from 1 to len(candidates for the question).\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for the current ranking.\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for rank in dup_ranks:\n",
    "        if rank <= k:\n",
    "            cnt+=1\n",
    "    result = cnt/ len(dup_ranks) \n",
    "    return result\n",
    "    ############################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of ranks of the duplicates; one rank per question; \n",
    "                   length is a number of questions that we check (N); \n",
    "                   rank is a number from 1 to len(candidates for the question).\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for the current ranking.\n",
    "    \"\"\"\n",
    "    cnt= 0\n",
    "    for rank in dup_ranks:\n",
    "        if rank <= k:\n",
    "            cnt+=1/np.log2(1+rank)\n",
    "    result = cnt/ len(dup_ranks) \n",
    "    return result \n",
    "    ############################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте функции. Успешное прохождение базовых тестов еще не гарантирует корректности реализации!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "from tests import test_hits\n",
    "print(test_hits(hits_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "from tests import test_dcg\n",
    "print(test_dcg(dcg_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ранжирование вопросов StackOverflow\n",
    "\n",
    "Выборка уже разбита на обучающую и тестовую. Все файлы используют табуляцию в качестве разделителя, но они имеют разный формат:\n",
    "\n",
    "- *обучающая* выборка (test.tsv) содержит похожие друг на друга предложения в каждой строке;\n",
    "- *тестовая* выборка (validation.tsv) содержит в каждой строке: *вопрос, похожий вопрос, отрицательный пример 1, отрицательный пример 2, ...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считайте тестовую выборку для оценки качества текущего решения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Реализуйте функцию ранжирования кандидатов на основе косинусного расстояния.**</font>\n",
    "    \n",
    "Функция должна по списку кандидатов вернуть отсортированный список пар (позиция в исходном списке кандидатов, кандидат). При этом позиция кандидата в полученном списке является его рейтингом (первый - лучший). Например, если исходный список кандидатов был [a, b, c], и самый похожий на исходный вопрос среди них - c, затем a, и в конце b, то функция должна вернуть список *[(2, c), (0, a), (1, b)]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    result = []\n",
    "    question = question_to_vec_by_mean(question,embeddings,dim).reshape(1,-1)\n",
    "    for rank,card in enumerate(candidates):\n",
    "        card_old = card\n",
    "        card = question_to_vec_by_mean(card,embeddings,dim).reshape(1,-1)\n",
    "        similarities.append((cosine_similarity(question,card),rank,card_old))\n",
    "    similarities = sorted(similarities,reverse=True)\n",
    "    for similarity in similarities:\n",
    "        result.append((similarity[1],similarity[2]) )\n",
    "    return result\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте работу функции на примерах ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "from tests import test_rank_candidates\n",
    "print(test_rank_candidates(rank_candidates, wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем оценить качество нашего метода. Запустите следующие два блока кода для получения результата. Обратите внимание, что вычисление расстояния между векторами занимает некоторое время (примерно 10 минут)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.203 | Hits@   1: 0.203\n",
      "DCG@   5: 0.259 | Hits@   5: 0.308\n",
      "DCG@  10: 0.277 | Hits@  10: 0.362\n",
      "DCG@ 100: 0.316 | Hits@ 100: 0.560\n",
      "DCG@ 500: 0.349 | Hits@ 500: 0.817\n",
      "DCG@1000: 0.368 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы проделали все шаги правильно, то вы должны разочароваться полученными результатами. Давайте попробуем понять, почему качество модели такое низкое. Когда вы работаете с какими-либо данными, очень полезно первым делом посмотреть на них глазами. Выведите несколько вопросов из наших данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Предобработка данных (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы могли заметить, мы имеем дело с сырыми данными. Это означает, что там присутствует много опечаток, спецсимволов и заглавных букв. В нашем случае это все может привести к ситуации, когда для данных токенов нет предобученных векторов. Поэтому необходима предобработка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Реализуйте функцию предобработки текстов.**</font>\n",
    "\n",
    "Вам требуется:\n",
    "- Перевести символы в нижний регистр;\n",
    "- Заменить символы пунктуации на пробелы;\n",
    "- Удалить \"плохие\" символы;\n",
    "- Удалить стопслова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/andrey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_eng = stopwords.words('english')\n",
    "reg = nltk.tokenize.RegexpTokenizer(r'\\w+') \n",
    "    \n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified string\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = reg.tokenize(text)\n",
    "    text = \" \".join([word for word in text if word not in stopwords_eng])\n",
    "    return text\n",
    "    \n",
    "    ###########################\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Теперь преобразуйте все вопросы из тестовой выборки. Оцените, как изменилось качество. Сделайте выводы.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3760/3760 [00:57<00:00, 65.85it/s]\n"
     ]
    }
   ],
   "source": [
    "new_validation = []\n",
    "\n",
    "for questions in tqdm_notebook(validation):\n",
    "    blocks = []\n",
    "    for quest in questions:\n",
    "        blocks.append(text_prepare(quest))  \n",
    "    new_validation.append(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3760/3760 [07:22<00:00,  8.49it/s]\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "wv_ranking_new = []\n",
    "for line in tqdm_notebook(new_validation):\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking_new.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.325 | Hits@   1: 0.325\n",
      "DCG@   5: 0.402 | Hits@   5: 0.470\n",
      "DCG@  10: 0.417 | Hits@  10: 0.517\n",
      "DCG@ 100: 0.453 | Hits@ 100: 0.695\n",
      "DCG@ 500: 0.473 | Hits@ 500: 0.850\n",
      "DCG@1000: 0.488 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking_new, k), k, hits_count(wv_ranking_new, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Представления для неизвестных слов. (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Оцените долю слов в выборке, для которых нет эмбеддинга в модели.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "def calc_proportion(validation,embeddings):\n",
    "    not_emb = []\n",
    "    emb = []\n",
    "    all_words = []\n",
    "    for questions in validation:\n",
    "        for quest in questions:\n",
    "            words = quest.split(' ')\n",
    "            for word in words:\n",
    "                all_words.append(word)\n",
    "                if word not in embeddings:\n",
    "                    not_emb.append(word)\n",
    "                else:\n",
    "                    emb.append(word)\n",
    "    return not_emb,emb,all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26633 8283101 38538 32106478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6910841247599772, 0.2579884657544811)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_emb,emb,all_words = calc_proportion(validation,wv_embeddings)\n",
    "print(len(set(not_emb)),len(not_emb),len(set(all_words)),len(all_words))\n",
    "len(set(not_emb))/len(set(all_words)),len(not_emb)/len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10233 3385749 18166 22495084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5633050754156116, 0.15051061823107661)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_emb,emb,all_words = calc_proportion(new_validation,wv_embeddings)\n",
    "print(len(set(not_emb)),len(not_emb),len(set(all_words)),len(all_words))\n",
    "len(set(not_emb))/len(set(all_words)),len(not_emb)/len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = list(set(emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, что получить представления для неизвестного слова, воспользуемся следующим подходом:\n",
    "    \n",
    "1. Будем восстанавливать эмбеддинг неизвестного слова как сумму эмбеддингов буквенных триграмм. Например, слово where должно представляться суммой триграмм _wh, whe, her, ere, re_\n",
    "\n",
    "2. В качестве обучающих данных будем использовать слова, для которых есть эмбеддинг в модели. Будем обучать эмбеддинги триграмм по выборке эмбеддингов с помощью функционала MSE:\n",
    "\n",
    "$$L = \\sum_{w \\in W_{known}}\\| f_{\\theta}(w) - v_w \\|^2 \\to \\min_{\\theta}$$\n",
    "\n",
    "где:\n",
    "\n",
    "* $W_{known}$ — множество известных модели слов\n",
    "* $f_{\\theta}(w)$ — сумма эмбеддингов триграмм слова $w$\n",
    "* $v_w$ — эмбеддинг слова $w$\n",
    "* $\\theta$ — веса эмбеддингов триграмм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Реализуйте предложенную модель ниже.**</font>\n",
    "\n",
    "Используйте класс nn.EmbeddingBag для построения среднего вектора представлений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_vocab = list(wv_embeddings.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500000it [00:00, 2019863.93it/s]\n"
     ]
    }
   ],
   "source": [
    "wv_vocab_all = {}\n",
    "for key,value in tqdm(zip(wv_vocab,wv_embeddings.vectors)):\n",
    "    wv_vocab_all[key]=value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0703125 ,  0.08691406,  0.08789062,  0.0625    ,  0.06933594,\n",
       "       -0.10888672, -0.08154297, -0.15429688,  0.02075195,  0.13183594,\n",
       "       -0.11376953, -0.03735352,  0.06933594,  0.078125  , -0.10302734,\n",
       "       -0.09765625,  0.04418945,  0.10253906, -0.06079102, -0.03613281,\n",
       "       -0.04541016,  0.04736328, -0.12060547, -0.06396484,  0.0022583 ,\n",
       "        0.03710938, -0.00291443,  0.11767578,  0.06176758,  0.06396484,\n",
       "        0.08105469, -0.06884766, -0.0213623 ,  0.05517578, -0.08544922,\n",
       "        0.06884766, -0.12792969, -0.03320312,  0.09863281,  0.17578125,\n",
       "        0.11083984, -0.03466797, -0.04711914, -0.00848389,  0.03588867,\n",
       "        0.10302734,  0.02697754, -0.02868652, -0.00512695,  0.10644531,\n",
       "        0.05981445,  0.09423828,  0.03369141, -0.02709961, -0.09423828,\n",
       "        0.00102997, -0.04833984,  0.03442383,  0.08105469, -0.11328125,\n",
       "       -0.08886719,  0.03588867, -0.14550781, -0.24414062, -0.06152344,\n",
       "        0.05297852,  0.05688477,  0.1796875 ,  0.06103516,  0.08691406,\n",
       "        0.12402344, -0.0402832 ,  0.02258301,  0.17773438, -0.02966309,\n",
       "       -0.02966309,  0.1171875 ,  0.03112793, -0.09619141,  0.06640625,\n",
       "        0.00469971, -0.08007812,  0.06298828, -0.02062988, -0.0546875 ,\n",
       "       -0.13574219, -0.06347656,  0.08349609, -0.06396484,  0.02148438,\n",
       "        0.07714844, -0.03710938, -0.03369141, -0.18359375, -0.07275391,\n",
       "        0.01586914,  0.09326172, -0.06152344, -0.01422119, -0.00344849,\n",
       "        0.0111084 , -0.15820312, -0.01708984,  0.00619507, -0.00872803,\n",
       "       -0.08056641, -0.01525879, -0.08789062,  0.003479  , -0.01611328,\n",
       "       -0.0123291 ,  0.09765625, -0.13964844, -0.0859375 , -0.02685547,\n",
       "        0.05395508,  0.1328125 ,  0.11279297,  0.12109375,  0.08544922,\n",
       "       -0.0071106 ,  0.04467773, -0.14550781, -0.00320435, -0.11767578,\n",
       "       -0.06542969,  0.07128906, -0.09423828, -0.03027344,  0.12011719,\n",
       "        0.08007812, -0.09472656, -0.16210938, -0.07763672,  0.02124023,\n",
       "       -0.08154297,  0.00393677, -0.15722656, -0.09814453,  0.03979492,\n",
       "        0.03930664, -0.00909424,  0.10302734,  0.06787109, -0.04272461,\n",
       "        0.06347656, -0.04907227,  0.02087402, -0.16699219,  0.09326172,\n",
       "        0.09375   ,  0.00686646,  0.05371094,  0.05249023, -0.02441406,\n",
       "       -0.0324707 , -0.06152344, -0.0055542 ,  0.09619141,  0.0378418 ,\n",
       "        0.01220703, -0.04394531, -0.00747681,  0.10546875,  0.02038574,\n",
       "        0.14550781,  0.08203125,  0.00576782,  0.00457764, -0.09277344,\n",
       "       -0.13867188, -0.05737305, -0.05151367, -0.13085938, -0.13964844,\n",
       "       -0.02050781, -0.02709961,  0.03271484,  0.10498047, -0.00233459,\n",
       "       -0.02258301,  0.00050354, -0.11083984,  0.08496094, -0.12988281,\n",
       "       -0.01745605, -0.00035858,  0.10791016,  0.08886719,  0.04467773,\n",
       "        0.02514648,  0.02380371,  0.08105469,  0.02368164, -0.10986328,\n",
       "        0.00537109, -0.0177002 , -0.03393555, -0.03295898, -0.1640625 ,\n",
       "        0.09570312, -0.01831055,  0.00531006, -0.03442383, -0.04418945,\n",
       "       -0.06640625, -0.01794434, -0.02966309, -0.00759888, -0.05126953,\n",
       "       -0.05419922,  0.08935547, -0.07177734,  0.01525879, -0.08251953,\n",
       "       -0.03173828,  0.03564453, -0.02124023, -0.05932617, -0.01306152,\n",
       "        0.046875  ,  0.02307129,  0.02099609, -0.07861328, -0.00805664,\n",
       "        0.01953125, -0.0055542 ,  0.04150391,  0.02783203,  0.01361084,\n",
       "        0.03466797, -0.18261719,  0.12011719,  0.07421875, -0.04101562,\n",
       "       -0.00994873,  0.04296875, -0.0072937 ,  0.12304688,  0.05761719,\n",
       "       -0.0534668 , -0.03222656, -0.00909424, -0.04663086,  0.04394531,\n",
       "       -0.05078125,  0.06884766,  0.00299072, -0.00418091, -0.04418945,\n",
       "        0.07373047, -0.01275635,  0.06738281,  0.00628662,  0.07519531,\n",
       "       -0.0378418 ,  0.00488281,  0.04467773, -0.06738281,  0.00970459,\n",
       "        0.00473022,  0.02050781,  0.07128906,  0.17089844,  0.17382812,\n",
       "        0.05566406,  0.09130859, -0.03735352,  0.04980469, -0.03930664,\n",
       "        0.04418945,  0.0625    ,  0.04858398, -0.05322266,  0.04882812,\n",
       "       -0.13085938, -0.02893066, -0.03613281, -0.06079102, -0.05737305,\n",
       "        0.12304688, -0.08251953, -0.01190186,  0.125     ,  0.00135803,\n",
       "        0.06396484, -0.10644531, -0.14355469, -0.04223633,  0.02404785,\n",
       "       -0.16894531, -0.08886719, -0.08056641,  0.06494141,  0.0612793 ,\n",
       "       -0.04736328, -0.05883789, -0.04760742,  0.01446533, -0.0625    ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_vocab_all['in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' wh', 'whe', 'her', 'ere', 're ']"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = \" where \"\n",
    "trigrams_check = [ (check[i]+ check[i + 1]+ check[i + 2])\n",
    "           for i in range(len(check) - 2)]\n",
    "trigrams_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pre', 'rec', 'eco', 'com', 'omp', 'mpi', 'pil', 'ile', 'pr', 'le']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = \"precompile\"\n",
    "trigrams_check = [ (check[i]+ check[i + 1]+ check[i + 2])\n",
    "           for i in range(len(check) - 2)]\n",
    "if len(check)>2:\n",
    "    trigrams_check.append(check[:2])\n",
    "    trigrams_check.append(check[-2:])\n",
    "if len(check) ==2 & (len(check[:2])==len(check[:-2])):\n",
    "    trigrams_check.append(check[:2])\n",
    "trigrams_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(wv_word):\n",
    "    trigrams = [ (wv_word[i]+ wv_word[i + 1]+ wv_word[i + 2]) for i in range(len(wv_word) - 2)]\n",
    "    if len(wv_word)>2:\n",
    "        trigrams.append(wv_word[:2])\n",
    "        trigrams.append(wv_word[-2:])\n",
    "    if len(wv_word) ==2 :\n",
    "        trigrams.append(wv_word[:2])\n",
    "    if len(wv_word) ==1 :\n",
    "        trigrams.append(wv_word[:2])\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigrammEmbeddingsModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_known_tokens, embedding_dim=300):\n",
    "        \"\"\"\n",
    "        all_known_tokens : list of str\n",
    "        \n",
    "        embedding_dim : int\n",
    "        \"\"\"\n",
    "        super(TrigrammEmbeddingsModel, self).__init__()\n",
    "        \n",
    "        emb_trigrams = []\n",
    "            \n",
    "        for w in all_known_tokens:\n",
    "            lst_trig = make_trigrams(w)\n",
    "            for trig in lst_trig:\n",
    "                emb_trigrams.append(trig)\n",
    "\n",
    "                \n",
    "        self.emb_trigrams = list(set(emb_trigrams))\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(len(self.emb_trigrams),embedding_dim, mode='sum')\n",
    "        \n",
    "        self.word_to_ix = {word: i for i, word in enumerate(self.emb_trigrams)}\n",
    "        \n",
    "        del emb_trigrams\n",
    "        \n",
    "\n",
    "    def forward(self, token):\n",
    "        trigrams = make_trigrams(token)\n",
    "        \n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in trigrams if w in self.emb_trigrams], dtype=torch.long)        \n",
    "        embedded = self.embedding(context_idxs,torch.tensor([0], dtype=torch.long))\n",
    "        return embedded\n",
    "        ###########################\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        ###########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>** Обучите модель. Оцените, как изменилось качество. Сделайте выводы.**</font>\n",
    "\n",
    "Если вы всё реализовали правильно, качество решения должно вырасти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "###########################\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "model = TrigrammEmbeddingsModel(emb)\n",
    "optimizer = optim.SGD(model.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7933"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4fc668440c4df5865aa73f2fedb9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8fb810e6074722b4ac40203ad23899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a482df178949f29cb1e67e108a7211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99ea70ebe8541d6aee85f17aef1c405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846aecaebe7b4c29bc8734d92e0de4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c0ca52421c4804b3531e68dac9b4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f874cb68bf34f4e94c926d28e0036b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2dba9250aa45eca05f2b0eaaa7ca20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb9c4103ad2466fbf93cd9008bdd343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6054e66120cf4c2db3eba81de087d010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cfc7fd3d4f4539adb3a790d53f46ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba73a96027d1450cbed3646ea428c134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2933327677f64579befa67c69754a2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbf0ceef28e4c5ca8fcf130517b05f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5180e8cf1c477599aa985af0edb480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bf0501e7e04e258458b7190efc6a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1299.5714678252823, 1268.3225867140268, 1238.8211302147167, 1210.900431827866, 1184.4173349788987, 1159.2476492996823, 1135.282774521426, 1112.427084687768, 1090.5958682817243, 1069.7136967614638, 1049.713094825509, 1037.605618326237, 1035.7000789981344, 1033.8023634775582, 1031.9124205630287]\n"
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "scheduler10 = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "for epoch in tqdm_notebook(range(15)):\n",
    "    total_loss = 0\n",
    "    for word_indx, word in tqdm_notebook(enumerate(emb)):\n",
    "        trigramm_emb_sum = model(word)\n",
    "\n",
    "        loss = loss_function(trigramm_emb_sum, torch.tensor([wv_embeddings[word]], dtype=torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    #         if word_indx % 100 == 0:\n",
    "    #             print(f'word {word} number {word_indx} of {len(emb)}, loss {loss.item()}' )\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    scheduler10.step()\n",
    "    total_loss = total_loss/len(emb)\n",
    "    losses.append(total_loss)\n",
    "print(losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.0226, -3.2949,  0.5479,  1.0509, -3.9814,  4.4350, -0.1516, -0.5238,\n",
       "          2.1994, -2.3503, -1.9312,  3.7317, -3.3457, -0.2678, -1.3114, -0.0884,\n",
       "          2.7414,  0.1291, -0.4851,  1.2593, -0.4343, -0.5687, -0.9252, -2.2965,\n",
       "         -1.1011, -2.9368,  0.4264, -0.4570,  2.8129, -2.6712,  2.1259,  1.6930,\n",
       "          0.0507,  1.1553,  0.7067,  3.9731, -1.5501, -1.7003,  0.1955,  1.2425,\n",
       "         -1.3060,  1.9096,  2.1361,  2.1357, -0.5932,  1.6000,  0.6950, -2.0958,\n",
       "          0.8221,  1.5572, -1.4201, -1.3339, -0.1526, -0.6479, -0.2864, -1.4138,\n",
       "          1.1786,  3.2894,  0.9411, -1.6700, -2.7773,  2.0487,  1.8302,  0.5926,\n",
       "         -1.2815,  0.5591, -5.9965,  1.7289,  2.5235, -0.5404, -0.1636,  0.3937,\n",
       "         -1.8177, -2.6313, -0.8833,  2.6657, -1.9473,  1.0772,  1.5346,  3.4262,\n",
       "         -0.3042, -1.3921,  2.4929, -4.6760,  2.3037, -3.0958,  2.4653,  2.6036,\n",
       "         -0.7576, -0.9465,  0.9372,  1.4302, -0.4282,  1.8936, -0.1807,  1.1568,\n",
       "         -1.2189,  0.2614,  1.1503,  3.1630, -1.3492, -0.9266, -0.4324, -0.1873,\n",
       "         -2.9939, -0.6464,  0.8157, -1.0578,  0.7884, -1.0449,  3.0722,  0.2257,\n",
       "         -2.4053,  0.0324, -1.6981, -1.9620,  2.4581, -1.0865,  2.2336, -1.2530,\n",
       "         -1.0698,  3.0793,  0.9460,  3.7559,  0.5108, -2.3484,  0.0364,  2.1534,\n",
       "         -4.4825,  0.8394,  1.5280, -0.5072, -0.2403,  2.1583,  0.2944, -2.4372,\n",
       "         -1.7346, -0.3045,  1.1860,  3.6930,  1.1597,  1.7178, -5.1200,  1.4931,\n",
       "          1.7319,  1.8017,  3.5086,  2.2652, -0.2065,  2.4660,  2.2485,  1.4901,\n",
       "          2.3424,  1.2031,  0.1330, -3.5673, -0.7303,  1.1505, -3.1933, -0.4508,\n",
       "          1.8946, -1.4406,  3.2360,  1.9521,  3.1467,  3.3107, -0.5550,  2.0945,\n",
       "         -0.2663,  2.5026,  0.0627,  1.6458, -3.2552, -1.3660,  0.6175,  2.1108,\n",
       "         -1.5088,  2.1344,  0.4514, -1.1419,  1.2308,  2.0310,  2.0607,  1.3198,\n",
       "          2.8053,  0.9810, -0.0648, -1.0018,  2.6591,  3.6052, -0.1102,  3.5240,\n",
       "         -2.2623, -0.8768,  0.9110,  2.7903,  1.7518,  0.6010, -0.0132, -3.0910,\n",
       "         -0.5941,  1.8101, -1.4718, -0.6320, -2.1485, -1.2047,  1.1861,  1.1035,\n",
       "         -2.1169, -1.5101, -2.2327,  0.8702,  2.2124, -4.3872,  0.9839,  1.2431,\n",
       "         -0.4392,  1.9425,  2.8990, -1.8655, -1.5041,  0.2255,  3.2766,  2.2057,\n",
       "         -0.8558, -2.1767,  1.2500,  0.0229,  1.4531,  0.4358,  0.5534,  2.9531,\n",
       "         -0.1272,  2.3659,  0.9361, -1.2647, -2.4422, -2.2559,  2.3668,  0.4384,\n",
       "         -2.0394,  0.8108,  4.2618, -2.2425,  0.0909, -1.0806, -0.0989, -0.9921,\n",
       "         -2.2938, -2.7826,  1.2824,  3.7645, -1.3680,  1.1217,  0.1812, -0.3867,\n",
       "          0.6113,  1.5682, -0.2569, -1.9971, -1.7627,  1.2797, -0.7167,  4.4463,\n",
       "          4.3642,  3.6478,  6.0387,  0.1345, -0.5634,  2.0991,  1.0751,  0.4971,\n",
       "         -1.6117,  3.3240,  0.6318, -0.4224,  0.2802, -0.8082, -2.1932,  0.9737,\n",
       "          1.1480,  2.0948,  0.2393, -1.7143, -1.7602, -1.0252,  1.7793,  1.2701,\n",
       "         -1.6101,  2.4911,  0.8116,  0.2244,  2.4742,  0.8108,  0.6978, -1.2564,\n",
       "         -3.5203,  0.1067, -2.4588, -3.1055]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model('recursion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4a21b91f9a45738437bd66ec4260ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10233), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "onemore_wv = {}\n",
    "for em in tqdm_notebook(set(not_emb)):\n",
    "    onemore_wv[em] =  model(em).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "204603it [00:00, 2039147.77it/s]\u001b[A\n",
      "393742it [00:00, 1965469.13it/s]\u001b[A\n",
      "500000it [00:00, 2009724.95it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "wv_vocab_all = {}\n",
    "for key,value in tqdm(zip(wv_vocab,wv_embeddings.vectors)):\n",
    "    wv_vocab_all[key]=value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_vocab_all.update(onemore_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e03091a84e748eea0b4054b2b3acbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3760), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wv_ranking_onemore = []\n",
    "for line in tqdm_notebook(new_validation):\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_vocab_all)\n",
    "    wv_ranking_onemore.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.378 | Hits@   1: 0.378\n",
      "DCG@   5: 0.447 | Hits@   5: 0.502\n",
      "DCG@  10: 0.459 | Hits@  10: 0.540\n",
      "DCG@ 100: 0.482 | Hits@ 100: 0.655\n",
      "DCG@ 500: 0.505 | Hits@ 500: 0.841\n",
      "DCG@1000: 0.522 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking_onemore, k), k, hits_count(wv_ranking_onemore, k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.377 | Hits@   1: 0.377\n",
      "DCG@   5: 0.444 | Hits@   5: 0.499\n",
      "DCG@  10: 0.455 | Hits@  10: 0.532\n",
      "DCG@ 100: 0.479 | Hits@ 100: 0.651\n",
      "DCG@ 500: 0.502 | Hits@ 500: 0.840\n",
      "DCG@1000: 0.519 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking_onemore, k), k, hits_count(wv_ranking_onemore, k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.374 | Hits@   1: 0.374\n",
      "DCG@   5: 0.442 | Hits@   5: 0.499\n",
      "DCG@  10: 0.452 | Hits@  10: 0.529\n",
      "DCG@ 100: 0.475 | Hits@ 100: 0.646\n",
      "DCG@ 500: 0.499 | Hits@ 500: 0.835\n",
      "DCG@1000: 0.516 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking_onemore, k), k, hits_count(wv_ranking_onemore, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть: векторные представления StarSpace (2 балла)\n",
    "\n",
    "В бонусной части вам предлгается обучить эмбеддинги специально для задачи поиска дубликатов с помощью пакета [StarSpace](https://github.com/facebookresearch/StarSpace). К сожалению, его нельзя запустить на Windows, поэтому в этом случае мы рекоммендуем использовать готовый [docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) с пошаговыми инструкциями или воспользоваться платформой google colab.\n",
    "\n",
    "Данная модель все еще представляет вопросы с помощью усреднения векторов слов, однако обучается по размеченной выборке пар близких вопросов. Это позволяет обучить вектора, которые лучше подходят для конкретной задачи. Напомним, что в модели word2vec обучение происходят по парам близких слов, и на этапе обучения модель ничего не знает о наших планах по их усреднению в пост-обработке.\n",
    "\n",
    "\n",
    "### Как выбрать  параметры модели?\n",
    "\n",
    "Ниже приведены некоторые рекомендации, с которых можно начать свои эксперименты.\n",
    "\n",
    "- Обучение на парах близких предложений соответствует режиму *trainMode = 3*.\n",
    "- Используйте метод оптимизации adagrad (параметр *adagrad=True*).\n",
    "- Установите длину фразы равной 1 (параметр *ngrams*), чтобы получить только вектора слов.\n",
    "- Не используйте большое количество эпох (5 должно быть достаточно).\n",
    "- Поэкспериментируйте с несколькими размерностями *dim* (например, от 100 до 300).\n",
    "- Для сравнения векторов используйте *косинусную меру*.\n",
    "- Установите *minCount* больше 1 (например, 2), если вы не хотите получить вектора для редко встречающихся слов.\n",
    "- Параметр *verbose=True* будет показывать вам прогресс процесса обучения.\n",
    "- Параметр *negSearchLimit* отвечает за число отрицательных примеров, которые используются в обучении, рекомендованное значение 10.\n",
    "- Для ускорения обучения мы рекоммендуем поставить *шаг обучения (learning rate)* равным 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>** Обучите вектора StarSpace для униграм на обучающей выборке. Не забудьте использовать предобработанную версию данных. **</font>\n",
    "\n",
    "Если вы следовали инструкциям правильно, то процесс обучения займет около 1 часа. Размер словаря полученных векторных представлений должен быть порядка 100000 (число строк в полученном файле). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data\n",
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_file(old_file, new_file):\n",
    "    new = open(new_file, 'w')\n",
    "    for questions in open(old_file, encoding='utf8'):\n",
    "        questions = questions.strip().split('\\t')\n",
    "        new_questions = []\n",
    "        for ques in questions:\n",
    "            new_questions.append(text_prepare(ques))\n",
    "        print(*new_questions, sep='\\t', file=new)\n",
    "    new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file('data/train.tsv', 'data/new_train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "batchSize: 5\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/train.tsv\n",
      "Read 19M words\n",
      "Number of words in dictionary:  205221\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/train.tsv\n",
      "Total number of examples loaded : 999799\n",
      "Initialized model weights. Model size :\n",
      "matrix : 205221 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: 0.040040  loss: 0.061931  eta: 0h3m  tot: 0h0m47s  (20.0%).074012  eta: 0h3m  tot: 0h0m32s  (13.0%)0.073338  eta: 0h3m  tot: 0h0m32s  (13.3%)\n",
      " ---+++                Epoch    0 Train error : 0.06212483 +++--- ☃\n",
      "Training epoch 1: 0.04 0.01\n",
      "Epoch: 100.0%  lr: 0.030010  loss: 0.013983  eta: 0h2m  tot: 0h1m29s  (40.0%)eta: 0h2m  tot: 0h0m51s  (21.5%)0h2m  tot: 0h1m6s  (28.4%)  tot: 0h1m6s  (28.5%)h2m  tot: 0h1m7s  (29.2%)2m  tot: 0h1m18s  (34.1%)\n",
      " ---+++                Epoch    1 Train error : 0.01382516 +++--- ☃\n",
      "Training epoch 2: 0.03 0.01\n",
      "Epoch: 100.0%  lr: 0.020260  loss: 0.008964  eta: 0h1m  tot: 0h2m9s  (60.0%) tot: 0h1m31s  (40.8%)0.023293  loss: 0.008967  eta: 0h1m  tot: 0h1m57s  (54.1%)59s  (54.9%)m  tot: 0h2m0s  (55.6%)m  tot: 0h2m2s  (56.5%)\n",
      " ---+++                Epoch    2 Train error : 0.00891135 +++--- ☃\n",
      "Training epoch 3: 0.02 0.01\n",
      "Epoch: 100.0%  lr: 0.010060  loss: 0.006908  eta: <1min   tot: 0h2m49s  (80.0%)6895  eta: 0h1m  tot: 0h2m13s  (61.5%)0.006722  eta: 0h1m  tot: 0h2m19s  (64.2%)  tot: 0h2m34s  (72.3%)0h2m40s  (75.2%)0.006876  eta: <1min   tot: 0h2m45s  (77.7%)\n",
      " ---+++                Epoch    3 Train error : 0.00685986 +++--- ☃\n",
      "Training epoch 4: 0.01 0.01\n",
      "Epoch: 100.0%  lr: 0.000380  loss: 0.005964  eta: <1min   tot: 0h3m28s  (100.0%)1s  (80.3%)0h2m51s  (80.4%)0.005981  eta: <1min   tot: 0h2m59s  (84.4%)  tot: 0h3m10s  (90.4%)0.005987  eta: <1min   tot: 0h3m24s  (97.9%)\n",
      " ---+++                Epoch    4 Train error : 0.00594504 +++--- ☃\n",
      "Saving model to file : StarSpace_emb\n",
      "Saving model in tsv format : StarSpace_emb.tsv\n"
     ]
    }
   ],
   "source": [
    "##### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ TRAINING HAPPENING HERE #####\n",
    "!./starspace train -trainFile \"data/train.tsv\" -model StarSpace_emb -trainMode 3 -adagrad true -ngrams 1 \\\n",
    "-epoch 5 -dim 100 -similarity cosine -minCount 2 -verbose true  -negSearchLimit 10 -lr 0.05 -fileFormat labelDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "batchSize: 5\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/new_train.tsv\n",
      "Read 13M words\n",
      "Number of words in dictionary:  73320\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/new_train.tsv\n",
      "Total number of examples loaded : 999910\n",
      "Initialized model weights. Model size :\n",
      "matrix : 73320 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: 0.040000  loss: 0.038340  eta: 0h2m  tot: 0h0m36s  (20.0%) tot: 0h0m2s  (1.4%)0.070352  eta: 0h3m  tot: 0h0m7s  (3.7%)m  tot: 0h0m18s  (9.7%)2m  tot: 0h0m21s  (11.3%)0.044184  loss: 0.045964  eta: 0h2m  tot: 0h0m22s  (11.8%)\n",
      " ---+++                Epoch    0 Train error : 0.03939438 +++--- ☃\n",
      "Training epoch 1: 0.04 0.01\n",
      "Epoch: 100.0%  lr: 0.030050  loss: 0.013155  eta: 0h1m  tot: 0h1m9s  (40.0%)0.012474  eta: 0h1m  tot: 0h0m41s  (22.7%)18.8%  lr: 0.038208  loss: 0.012796  eta: 0h1m  tot: 0h0m42s  (23.8%)20.9%  lr: 0.037998  loss: 0.012745  eta: 0h1m  tot: 0h0m43s  (24.2%)0.013349  eta: 0h1m  tot: 0h0m48s  (26.9%)%  lr: 0.035476  loss: 0.013448  eta: 0h1m  tot: 0h0m51s  (28.7%)0.013340  eta: 0h1m  tot: 0h1m0s  (34.2%)0.013362  eta: 0h1m  tot: 0h1m2s  (35.4%)0.032122  loss: 0.013331  eta: 0h1m  tot: 0h1m2s  (35.6%)0h1m  tot: 0h1m7s  (38.7%)\n",
      " ---+++                Epoch    1 Train error : 0.01305809 +++--- ☃\n",
      "Training epoch 2: 0.03 0.01\n",
      "Epoch: 100.0%  lr: 0.020220  loss: 0.009496  eta: 0h1m  tot: 0h1m42s  (60.0%)  tot: 0h1m10s  (40.1%)%  lr: 0.021131  loss: 0.009468  eta: 0h1m  tot: 0h1m39s  (58.0%)0.021051  loss: 0.009451  eta: 0h1m  tot: 0h1m39s  (58.2%)0.020401  loss: 0.009492  eta: 0h1m  tot: 0h1m41s  (59.5%)\n",
      " ---+++                Epoch    2 Train error : 0.00961004 +++--- ☃\n",
      "Training epoch 3: 0.02 0.01\n",
      "Epoch: 100.0%  lr: 0.010541  loss: 0.008035  eta: <1min   tot: 0h2m14s  (80.0%)ot: 0h1m44s  (60.7%)  eta: 0h1m  tot: 0h1m49s  (63.3%)  loss: 0.007858  eta: <1min   tot: 0h1m52s  (65.4%)0h2m1s  (71.0%)0.007960  eta: <1min   tot: 0h2m1s  (71.1%)0h2m1s  (71.2%)  loss: 0.008013  eta: <1min   tot: 0h2m7s  (75.1%)0.008011  eta: <1min   tot: 0h2m7s  (75.4%)  tot: 0h2m10s  (77.3%)0h2m14s  (80.0%)\n",
      " ---+++                Epoch    3 Train error : 0.00793345 +++--- ☃\n",
      "Training epoch 4: 0.01 0.01\n",
      "Epoch: 100.0%  lr: 0.000270  loss: 0.006833  eta: <1min   tot: 0h2m48s  (100.0%)ot: 0h2m20s  (82.0%)0h2m21s  (82.8%)0.006867  eta: <1min   tot: 0h2m22s  (83.0%)s  (89.8%)2m37s  (92.2%)0.003183  loss: 0.006771  eta: <1min   tot: 0h2m39s  (93.1%)  eta: <1min   tot: 0h2m45s  (97.7%)\n",
      " ---+++                Epoch    4 Train error : 0.00712226 +++--- ☃\n",
      "Saving model to file : StarSpace_emb_new\n",
      "Saving model in tsv format : StarSpace_emb_new.tsv\n"
     ]
    }
   ],
   "source": [
    "##### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ TRAINING HAPPENING HERE #####\n",
    "!./starspace train -trainFile \"data/new_train.tsv\" -model StarSpace_emb_new -trainMode 3 -adagrad true -ngrams 1 \\\n",
    "-epoch 5 -dim 100 -similarity cosine -minCount 2 -verbose true  -negSearchLimit 10 -lr 0.05 -fileFormat labelDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 300\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "batchSize: 5\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/new_train.tsv\n",
      "Read 13M words\n",
      "Number of words in dictionary:  73320\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/new_train.tsv\n",
      "Total number of examples loaded : 999910\n",
      "Initialized model weights. Model size :\n",
      "matrix : 73320 300\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: 0.040040  loss: 0.038182  eta: 0h8m  tot: 0h2m12s  (20.0%)  lr: 0.047608  loss: 0.066575  eta: 0h12m  tot: 0h0m37s  (4.7%)0.047247  loss: 0.062591  eta: 0h11m  tot: 0h0m41s  (5.5%)%  lr: 0.046036  loss: 0.054014  eta: 0h10m  tot: 0h0m55s  (8.0%)h0m56s  (8.1%)%  lr: 0.045956  loss: 0.053537  eta: 0h10m  tot: 0h0m56s  (8.2%)0h10m  tot: 0h0m57s  (8.2%)10m  tot: 0h0m59s  (8.7%)0.046192  eta: 0h9m  tot: 0h1m16s  (11.8%)h1m37s  (14.4%)0.040430  eta: 0h9m  tot: 0h1m58s  (17.1%)0.040951  loss: 0.040166  eta: 0h9m  tot: 0h1m59s  (17.4%)0.038744  eta: 0h8m  tot: 0h2m7s  (19.2%)\n",
      " ---+++                Epoch    0 Train error : 0.03900142 +++--- ☃\n",
      "Training epoch 1: 0.04 0.01\n",
      "Epoch: 100.0%  lr: 0.030110  loss: 0.012621  eta: 0h6m  tot: 0h4m16s  (40.0%).037217  loss: 0.012319  eta: 0h8m  tot: 0h2m52s  (25.5%)0.012436  eta: 0h8m  tot: 0h3m1s  (26.9%)0.012410  eta: 0h8m  tot: 0h3m4s  (27.3%)h8m  tot: 0h3m14s  (28.7%)0.033884  loss: 0.012399  eta: 0h7m  tot: 0h3m38s  (32.4%)0.033824  loss: 0.012390  eta: 0h7m  tot: 0h3m40s  (32.6%)  tot: 0h3m45s  (33.5%)0.012500  eta: 0h7m  tot: 0h3m49s  (34.4%)m  tot: 0h4m1s  (36.7%)0.031031  loss: 0.012596  eta: 0h6m  tot: 0h4m8s  (38.1%)0h6m  tot: 0h4m15s  (39.7%)m  tot: 0h4m16s  (39.8%)\n",
      " ---+++                Epoch    1 Train error : 0.01262625 +++--- ☃\n",
      "Training epoch 2: 0.03 0.01\n",
      "Epoch: 100.0%  lr: 0.020000  loss: 0.008286  eta: 0h3m  tot: 0h6m10s  (60.0%)m23s  (40.9%)0.025656  loss: 0.008023  eta: 0h4m  tot: 0h5m1s  (48.4%)0.024875  loss: 0.008037  eta: 0h4m  tot: 0h5m8s  (49.9%)0.024675  loss: 0.008061  eta: 0h4m  tot: 0h5m10s  (50.3%)0.008120  eta: 0h4m  tot: 0h5m12s  (50.9%)4m  tot: 0h5m50s  (56.5%)0.008310  eta: 0h3m  tot: 0h6m5s  (58.9%)\n",
      " ---+++                Epoch    2 Train error : 0.00834745 +++--- ☃\n",
      "Training epoch 3: 0.02 0.01\n",
      "Epoch: 100.0%  lr: 0.010080  loss: 0.006270  eta: 0h1m  tot: 0h7m57s  (80.0%) eta: 0h3m  tot: 0h6m23s  (62.1%)  eta: 0h3m  tot: 0h6m28s  (63.2%)3m  tot: 0h7m2s  (68.4%)63.9%  lr: 0.013764  loss: 0.006179  eta: 0h2m  tot: 0h7m23s  (72.8%)0.006200  eta: 0h2m  tot: 0h7m30s  (74.2%)\n",
      " ---+++                Epoch    3 Train error : 0.00641872 +++--- ☃\n",
      "Training epoch 4: 0.01 0.01\n",
      "Epoch: 100.0%  lr: 0.000471  loss: 0.005452  eta: <1min   tot: 0h9m41s  (100.0%)  tot: 0h8m24s  (85.5%)  loss: 0.005619  eta: 0h1m  tot: 0h8m24s  (85.5%)30.8%  lr: 0.007347  loss: 0.005603  eta: 0h1m  tot: 0h8m27s  (86.2%)48.3%  lr: 0.005726  loss: 0.005563  eta: <1min   tot: 0h8m42s  (89.7%)  tot: 0h8m45s  (90.5%)60.6%  lr: 0.004595  loss: 0.005499  eta: <1min   tot: 0h8m53s  (92.1%)0.003564  loss: 0.005495  eta: <1min   tot: 0h9m7s  (94.1%)h9m8s  (94.2%)m29s  (97.6%)34s  (98.7%)0.001011  loss: 0.005432  eta: <1min   tot: 0h9m36s  (99.0%)\n",
      " ---+++                Epoch    4 Train error : 0.00548331 +++--- ☃\n",
      "Saving model to file : StarSpace_emb_new_300\n",
      "Saving model in tsv format : StarSpace_emb_new_300.tsv\n"
     ]
    }
   ],
   "source": [
    "##### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ TRAINING HAPPENING HERE #####\n",
    "!./starspace train -trainFile \"data/new_train.tsv\" -model StarSpace_emb_new_300 -trainMode 3 -adagrad true -ngrams 1 \\\n",
    "-epoch 5 -dim 300 -similarity cosine -minCount 2 -verbose true  -negSearchLimit 10 -lr 0.05 -fileFormat labelDoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже вы можете проверить качество работы вашей модели. Так как обучение происходило для конкретной задачи на размеченных данных, то ожидается, что это решение будет иметь более высокое качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_starspace(filename):\n",
    "    starspace_embedding = {}\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        starspace_embedding[line[0]] = np.array(line[1:]).astype(np.float32)\n",
    "    return starspace_embedding\n",
    "starspace_embeddings = read_starspace('StarSpace_emb.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings_new = read_starspace('StarSpace_emb_new.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings_new_300 = read_starspace('StarSpace_emb_new_300.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in tqdm(new_validation):\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.473 | Hits@   1: 0.473\n",
      "DCG@   5: 0.578 | Hits@   5: 0.668\n",
      "DCG@  10: 0.597 | Hits@  10: 0.728\n",
      "DCG@ 100: 0.630 | Hits@ 100: 0.887\n",
      "DCG@ 500: 0.642 | Hits@ 500: 0.977\n",
      "DCG@1000: 0.644 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                              k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe2e295ef23427c8e10ed1c9bbfbbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3760), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ss_prepared_ranking_new = []\n",
    "for line in tqdm_notebook(new_validation):\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings_new, 100)\n",
    "    ss_prepared_ranking_new.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.528 | Hits@   1: 0.528\n",
      "DCG@   5: 0.633 | Hits@   5: 0.720\n",
      "DCG@  10: 0.652 | Hits@  10: 0.778\n",
      "DCG@ 100: 0.682 | Hits@ 100: 0.925\n",
      "DCG@ 500: 0.690 | Hits@ 500: 0.985\n",
      "DCG@1000: 0.692 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking_new, k), \n",
    "                                              k, hits_count(ss_prepared_ranking_new, k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d61024d6244fcd9866e9d20bd6b485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3760), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ss_prepared_ranking_new_300 = []\n",
    "for line in tqdm_notebook(new_validation):\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings_new_300, 300)\n",
    "    ss_prepared_ranking_new_300.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.527 | Hits@   1: 0.527\n",
      "DCG@   5: 0.638 | Hits@   5: 0.729\n",
      "DCG@  10: 0.656 | Hits@  10: 0.785\n",
      "DCG@ 100: 0.685 | Hits@ 100: 0.923\n",
      "DCG@ 500: 0.693 | Hits@ 500: 0.985\n",
      "DCG@1000: 0.694 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking_new_300, k), \n",
    "                                              k, hits_count(ss_prepared_ranking_new_300, k)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
