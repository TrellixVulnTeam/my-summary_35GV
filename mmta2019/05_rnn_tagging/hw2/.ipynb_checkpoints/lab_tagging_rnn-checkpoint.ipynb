{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Практическое задание 2 (часть 2)\n",
    "\n",
    "# Распознавание именованных сущностей из Twitter с помощью LSTM\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: Власов Андрей Валерьевич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "### Постановка задачи\n",
    "\n",
    "В этом задании вы будете использовать рекуррентные нейронные сети для решения проблемы распознавания именованных сущностей (NER). Примерами именованных сущностей являются имена людей, названия организаций, адреса и т.д. В этом задании вы будете работать с данными twitter.\n",
    "\n",
    "Например, вы хотите извлечь имена и названия организаций. Тогда для текста\n",
    "\n",
    "    Yan Goodfellow works for Google Brain\n",
    "\n",
    "модель должна извлечь следующую последовательность:\n",
    "\n",
    "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "\n",
    "где префиксы *B-* и *I-* означают начало и конец именованной сущности, *O* означает слово без тега. Такая префиксная система введена, чтобы различать последовательные именованные сущности одного типа.\n",
    "\n",
    "Решение этого задания будет основано на нейронных сетях, а именно на Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs).\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    " - [Pytorch](https://pytorch.org/).\n",
    " - [Numpy](http://www.numpy.org).\n",
    " \n",
    "### Данные\n",
    "\n",
    "Все данные содержатся в папке `./data`: `./data/train.txt`, `./data/validation.txt`, `./data/test.txt`.\n",
    "\n",
    "Скачать архив можно здесь: [ссылка на google диск](https://drive.google.com/open?id=1s1rFOFMZTBqtJuQDcIvW-8djA78iUDcx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Подготовка данных (2 балла)\n",
    "\n",
    "### Загрузка данных\n",
    "\n",
    "Мы будем работать с данными, которые содержат твиты с тегами именованных сущностей. Каждая строка файла содержит пару токен(слово или пунктуация) и тег, разделенные пробелом. Различные твиты разделены пустой строкой.\n",
    "\n",
    "Функция *read_data* считывает корпус из *file_path* и возвращает два списка: один с токенами и один с соответствующими токенам тегами. Также она заменяет все ники (токены, которые начинаются на символ *@*) на токен `<USR>` и url-ы (токены, которые начинаются на *http://* или *https://*) на токен `<URL>`. \n",
    "\n",
    "Вам необходимо реализовать эту функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_toks = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path,'r', encoding='utf-8'):\n",
    "        line = line.strip().split(' ')\n",
    "        if line==['']:\n",
    "            if tweet_toks != ['']:\n",
    "                tokens.append(tweet_toks)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_toks = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            if line[0].startswith('@'):\n",
    "                line[0] = '<USR>'\n",
    "            elif line[0].startswith('http://') or line[0].startswith('https://'):\n",
    "                line[0] = '<URL>'\n",
    "            tweet_toks.append(line[0])\n",
    "            tweet_tags.append(line[1])\n",
    "\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем загрузить 3 части данных:\n",
    " - *train* для тренировки модели;\n",
    " - *validation* для валидации и подбора гиперпараметров;\n",
    " - *test* для финального тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('data/train.txt')\n",
    "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
    "test_tokens, test_tags = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5795, 724, 724)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens),len(validation_tokens),len(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всегда полезно знать, с какими данными вы работаете. Выведем небольшую часть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT\tO\n",
      "<USR>\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n",
      "\n",
      "Apple\tB-product\n",
      "MacBook\tI-product\n",
      "Pro\tI-product\n",
      "A1278\tI-product\n",
      "13.3\tI-product\n",
      "\"\tI-product\n",
      "Laptop\tI-product\n",
      "-\tI-product\n",
      "MD101LL/A\tI-product\n",
      "(\tO\n",
      "June\tO\n",
      ",\tO\n",
      "2012\tO\n",
      ")\tO\n",
      "-\tO\n",
      "Full\tO\n",
      "read\tO\n",
      "by\tO\n",
      "eBay\tB-company\n",
      "<URL>\tO\n",
      "<URL>\tO\n",
      "\n",
      "Happy\tO\n",
      "Birthday\tO\n",
      "<USR>\tO\n",
      "!\tO\n",
      "May\tO\n",
      "Allah\tB-person\n",
      "s.w.t\tO\n",
      "bless\tO\n",
      "you\tO\n",
      "with\tO\n",
      "goodness\tO\n",
      "and\tO\n",
      "happiness\tO\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка словарей\n",
    "\n",
    "Чтобы обучать нейронную сеть, мы будем использовать два отображения.\n",
    "\n",
    "- {token}$\\to${token id}: устанавливает соответствие между токеном и строкой в embedding матрице;\n",
    "- {tag}$\\to${tag id}: one hot encoding тегов.\n",
    "\n",
    "\n",
    "Теперь вам необходимо реализовать функцию *build_dict*, которая должна возвращать словарь {token or tag}$\\to${index} и контейнер, задающий обратное отображение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "    tokens_or_tags: a list of lists of tokens or tags\n",
    "    special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "    # Create mappings from tokens to indices and vice versa\n",
    "    # Add special tokens to dictionaries\n",
    "    # The first special token must have index 0\n",
    "\n",
    "    cnt = 0\n",
    "    \n",
    "    for el in special_tokens:\n",
    "        tok2idx[el] = cnt\n",
    "        cnt += 1\n",
    "        idx2tok.append(el)\n",
    "    \n",
    "    for tokens in tokens_or_tags:\n",
    "        for token in tokens:\n",
    "            if token not in tok2idx:\n",
    "                tok2idx[token] = cnt\n",
    "                cnt += 1\n",
    "                idx2tok.append(token)    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После реализации функции *build_dict* вы можете создать словари для токенов и тегов. В нашем случае специальными токенами будут:\n",
    " - `<UNK>` токен для обозначаения слов, которых нет в словаре;\n",
    " - `<PAD>` токен для дополнения предложений одного батча до одинаковой длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генератор батчей\n",
    "\n",
    "Обычно нейронные сети обучаются батчами. Это означает, что каждое обновление весов нейронной сети происходит на основе нескольких последовательностей. Технической деталью является необходимость дополнить все последовательности внутри батча до одной длины. Для некоторых фреймворков (таких как tensorflow) это необходимо сделать до подачи батча в нейронную сеть. В случае с pytorch это можно сделать как вне архитектуры нейронной сети, так и внутри. Мы выбрали более универсальный вариант и наш генератор батчей дополняет все последовательности внутри одного батча до одной длины.\n",
    "\n",
    "Генератор батчей разбивает последовательность входных предложений и тегов на батчи размера batch_size. Размер последнего батча может быть меньше, если allow_smaller_last_batch is True, иначе последний батч исключается из генератора. Если включён параметр shuffle, данные перед разделением на батчи будут перемешаны. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, tokens_idxs, tags_idxs,\n",
    "                      shuffle=True, allow_smaller_last_batch=True, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generates padded batches of tags_idxs and tags_idxs.\n",
    "    \n",
    "    batch_size : int, number of objects in one batch\n",
    "    tokens_idxs : list of list of int\n",
    "    tags_idxs : list of list of int\n",
    "    shuffle : bool\n",
    "    allow_smaller_last_batch : bool\n",
    "    device: str, cpu or cuda:x\n",
    "    \n",
    "    yield x, y: torch.LongTensor and torch.LongTensor\n",
    "    x - batch of tokens_idxs, y - batch of tags_idxs\n",
    "    \"\"\"\n",
    "    n_samples = len(tokens_idxs)\n",
    "    \n",
    "    # Shuffle data if shuffle is True.\n",
    "    # Don't modify the original tokens and tags!\n",
    "    \n",
    "    if shuffle:\n",
    "        permutes = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        permutes = np.arange(n_samples)\n",
    "        \n",
    "    # Get the number of batches\n",
    "    \n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches =  n_samples // batch_size +1\n",
    "    else:\n",
    "        n_batches =  n_samples // batch_size\n",
    "        \n",
    "    # For each k yield pair x and y\n",
    "    for k in range(n_batches):\n",
    "        start_idx = k*batch_size\n",
    "        end_idx = min((k + 1) * batch_size, n_samples)\n",
    "        \n",
    "        cur_size = end_idx - start_idx\n",
    "        \n",
    "        x_ = []\n",
    "        y_ = []\n",
    "        mx_ln_tkns = 0\n",
    "        for idx in permutes[start_idx: end_idx]:\n",
    "            x_.append([token2idx[word] for word in tokens_idxs[idx]])\n",
    "            y_.append([tag2idx[tag] for tag in tags_idxs[idx]])\n",
    "            mx_ln_tkns = max(mx_ln_tkns, len(tags_idxs[idx]))\n",
    "        \n",
    "        x = np.ones([cur_size, mx_ln_tkns], dtype=np.long) * token2idx['<PAD>']\n",
    "        y = np.ones([cur_size, mx_ln_tkns], dtype=np.long) * tag2idx['O']\n",
    "        \n",
    "        for n in range(cur_size):\n",
    "            x[n, :len(x_[n])] = x_[n]\n",
    "            y[n, :len(x_[n])] = y_[n]\n",
    "              \n",
    "        \n",
    "        x = torch.from_numpy(x).long().to(device)\n",
    "        y = torch.from_numpy(y).long().to(device)\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте ваш генератор батчей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nonrandom_batch_generator = batches_generator(\n",
    "    batch_size=3,\n",
    "    tokens_idxs=train_tokens[:7],\n",
    "    tags_idxs=train_tags[:7],\n",
    "    shuffle=False,\n",
    "    allow_smaller_last_batch=True\n",
    ")\n",
    "\n",
    "batch_lengths = [3, 3, 1]\n",
    "sequence_lengths = [26, 25, 8]\n",
    "some_pad_tensor = torch.LongTensor([token2idx['<PAD>']] * 12)\n",
    "some_outside_tensor = torch.LongTensor([tag2idx['O'] * 12])\n",
    "\n",
    "for i, (tokens_batch, tags_batch) in enumerate(test_nonrandom_batch_generator):\n",
    "    assert tokens_batch.dtype == torch.int64, 'tokens_batch is not LongTensor'\n",
    "    assert tags_batch.dtype == torch.int64, 'tags_batch is not LongTensor'\n",
    "    \n",
    "    assert len(tokens_batch) == batch_lengths[i], 'wrong batch length'\n",
    "    \n",
    "    for one_token_sequence in tokens_batch:\n",
    "        assert len(one_token_sequence) == sequence_lengths[i], 'wrong length of sequence in batch'\n",
    "    \n",
    "    if i == 0:\n",
    "        assert torch.all(tokens_batch[2][-12:] == some_pad_tensor), \"wrong padding\"       \n",
    "        assert torch.all(tags_batch[2][-12:] == some_outside_tensor), \"wrong O tag\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. BiLSTM (3 балла)\n",
    "\n",
    "Определите архитектуру сети, используя библиотеку pytorch. \n",
    "\n",
    "**Замечания:**\n",
    "1. Для улучшения качества сети предлагается использовать дополнительный Embedding слой на входе (каждому слову ставится в соответствие обучаемый вектор). \n",
    "\n",
    "2. Не забудьте, что `<PAD>` токены не должны учавствовать в подсчёте функции потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel(torch.nn.Module):\n",
    "    def __init__(self, vocabulary_size, n_tags, PAD_index,\n",
    "                 embedding_dim, rnn_hidden_size,\n",
    "                 dropout_zeroed_probability,\n",
    "                 device='cpu'):\n",
    "        '''\n",
    "        Defines neural network structure.\n",
    "        \n",
    "        architecture: input -> Embedding -> BiLSTM -> Dropout -> Linear\n",
    "        optimizer: Adam\n",
    "        \n",
    "        ----------\n",
    "        Parameters\n",
    "        \n",
    "        vocabulary_size: int, number of words in vocabulary.\n",
    "        n_tags: int, number of tags.\n",
    "        PAD_index: int, index of padding character. Used for loss masking.\n",
    "        embedding_dim: int, dimension of words' embeddings.\n",
    "        rnn_hidden_size: int, number of hidden units in each LSTM cell\n",
    "        dropout_zeroed_probability: float, dropout zeroed probability for Dropout layer.\n",
    "        device: str, cpu or cuda:x\n",
    "        '''\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.n_tags = n_tags\n",
    "        self.embed = nn.Embedding(vocabulary_size, embedding_dim,PAD_index)\n",
    "        self.lstm = nn.LSTM(embedding_dim, rnn_hidden_size, num_layers = 2,\n",
    "                            batch_first=True, bidirectional=True,dropout=dropout_zeroed_probability)\n",
    "        self.linear = nn.Linear(rnn_hidden_size*2, n_tags) # 2 for bidirection\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        '''\n",
    "        Makes forward pass.\n",
    "        \n",
    "        ----------\n",
    "        Parameters\n",
    "        x_batch: torch.LongTensor with shape (number of samples in batch, number words in sentence).\n",
    "        '''\n",
    "        # Embed word ids to vectors\n",
    "        x_batch = self.embed(x_batch)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x_batch)\n",
    "        \n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out.to(self.device)\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict_for_batch(self, x_batch):\n",
    "        '''\n",
    "        Returns predictions for x_batch.\n",
    "        \n",
    "        return type: torch.LongTensor\n",
    "        return shape: (number of samples in batch, number words in sentence.\n",
    "        \n",
    "        ----------\n",
    "        Parameters\n",
    "        x_batch: torch.LongTensor with shape (number of samples in batch, number words in sentence).\n",
    "        '''\n",
    "        return nn.functional.softmax(self.forward(x_batch), dim=2).argmax(dim=2)\n",
    "        \n",
    "    \n",
    "    def train_on_batch(self, x_batch, y_batch, optimizer, loss_function,clipping=False):\n",
    "        '''\n",
    "        Trains model on the given batch.\n",
    "        \n",
    "        ----------\n",
    "        Parameters\n",
    "        x_batch: np.ndarray with shape (number of samples in batch, number words in sentence).\n",
    "        y_batch: np.ndarray with shape (number of samples in batch).\n",
    "        optimizer: torch.optimizer class\n",
    "        loss_function: torch loss class\n",
    "        '''\n",
    "         # zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = self.forward(x_batch).view(-1,self.n_tags)\n",
    "        \n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = loss_function(y_pred, y_batch.view(-1))\n",
    "        \n",
    "        # calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # update the parameters of the model\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "dropout_zeroed_probability = 0.8\n",
    "embedding_dim = 200\n",
    "rnn_hidden_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(idx2token)\n",
    "n_tags = len(idx2tag)\n",
    "PAD_index = token2idx['<PAD>']\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3a105abf794091b3b882e068661fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 3 phrases; correct: 0.\n",
      "\n",
      "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 0 phrases; correct: 0.\n",
      "\n",
      "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
      "\n",
      "epoch = 1 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 342 phrases; correct: 68.\n",
      "\n",
      "precision:  19.88%; recall:  1.51%; F1:  2.82\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 31 phrases; correct: 1.\n",
      "\n",
      "precision:  3.23%; recall:  0.19%; F1:  0.35\n",
      "\n",
      "epoch = 2 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 969 phrases; correct: 266.\n",
      "\n",
      "precision:  27.45%; recall:  5.93%; F1:  9.75\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 92 phrases; correct: 16.\n",
      "\n",
      "precision:  17.39%; recall:  2.98%; F1:  5.09\n",
      "\n",
      "epoch = 3 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 1941 phrases; correct: 645.\n",
      "\n",
      "precision:  33.23%; recall:  14.37%; F1:  20.06\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 193 phrases; correct: 51.\n",
      "\n",
      "precision:  26.42%; recall:  9.50%; F1:  13.97\n",
      "\n",
      "epoch = 4 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 2670 phrases; correct: 1042.\n",
      "\n",
      "precision:  39.03%; recall:  23.21%; F1:  29.11\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 271 phrases; correct: 79.\n",
      "\n",
      "precision:  29.15%; recall:  14.71%; F1:  19.55\n",
      "\n",
      "epoch = 5 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 2965 phrases; correct: 1249.\n",
      "\n",
      "precision:  42.12%; recall:  27.82%; F1:  33.51\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 297 phrases; correct: 84.\n",
      "\n",
      "precision:  28.28%; recall:  15.64%; F1:  20.14\n",
      "\n",
      "epoch = 6 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 3370 phrases; correct: 1589.\n",
      "\n",
      "precision:  47.15%; recall:  35.40%; F1:  40.44\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 324 phrases; correct: 96.\n",
      "\n",
      "precision:  29.63%; recall:  17.88%; F1:  22.30\n",
      "\n",
      "epoch = 7 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 3662 phrases; correct: 1743.\n",
      "\n",
      "precision:  47.60%; recall:  38.83%; F1:  42.77\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 380 phrases; correct: 108.\n",
      "\n",
      "precision:  28.42%; recall:  20.11%; F1:  23.56\n",
      "\n",
      "epoch = 8 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 3610 phrases; correct: 1903.\n",
      "\n",
      "precision:  52.71%; recall:  42.39%; F1:  46.99\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 371 phrases; correct: 116.\n",
      "\n",
      "precision:  31.27%; recall:  21.60%; F1:  25.55\n",
      "\n",
      "epoch = 9 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 4036 phrases; correct: 2122.\n",
      "\n",
      "precision:  52.58%; recall:  47.27%; F1:  49.78\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 426 phrases; correct: 105.\n",
      "\n",
      "precision:  24.65%; recall:  19.55%; F1:  21.81\n",
      "\n",
      "epoch = 10 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 4253 phrases; correct: 2327.\n",
      "\n",
      "precision:  54.71%; recall:  51.84%; F1:  53.24\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 466 phrases; correct: 109.\n",
      "\n",
      "precision:  23.39%; recall:  20.30%; F1:  21.73\n",
      "\n",
      "epoch = 11 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 4323 phrases; correct: 2481.\n",
      "\n",
      "precision:  57.39%; recall:  55.27%; F1:  56.31\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 451 phrases; correct: 114.\n",
      "\n",
      "precision:  25.28%; recall:  21.23%; F1:  23.08\n",
      "\n",
      "epoch = 12 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 4247 phrases; correct: 2634.\n",
      "\n",
      "precision:  62.02%; recall:  58.68%; F1:  60.30\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 442 phrases; correct: 119.\n",
      "\n",
      "precision:  26.92%; recall:  22.16%; F1:  24.31\n",
      "\n",
      "epoch = 13 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 4368 phrases; correct: 2803.\n",
      "\n",
      "precision:  64.17%; recall:  62.44%; F1:  63.29\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 471 phrases; correct: 118.\n",
      "\n",
      "precision:  25.05%; recall:  21.97%; F1:  23.41\n",
      "\n",
      "epoch = 14 loss= tensor(24.3753, grad_fn=<AddBackward0>)\n",
      "Train\n",
      "processed 105778 tokens with 4489 phrases; found: 4643 phrases; correct: 2958.\n",
      "\n",
      "precision:  63.71%; recall:  65.89%; F1:  64.78\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 554 phrases; correct: 122.\n",
      "\n",
      "precision:  22.02%; recall:  22.72%; F1:  22.36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bi_nn = BiLSTMModel(vocab_size, n_tags, PAD_index,\n",
    "                 embedding_dim, rnn_hidden_size,\n",
    "                 dropout_zeroed_probability,)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bi_nn.parameters(), lr=learning_rate)\n",
    "\n",
    "# number of epoch to train\n",
    "n_epoch = 15\n",
    "for epoch in tqdm_notebook(range(n_epoch)):\n",
    "    learning_rate/= 1.1\n",
    "    optimizer = torch.optim.Adam(bi_nn.parameters(), lr=learning_rate)\n",
    "    for batch_num, (X_batch, y_batch) in enumerate(batches_generator(batch_size, train_tokens, train_tags)):\n",
    "        bi_nn.train_on_batch(X_batch, y_batch, optimizer, loss_function) \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        print('epoch =',epoch)\n",
    "        print('Train')\n",
    "        eval_conll(bi_nn, train_tokens, train_tags)\n",
    "        print('Validation')\n",
    "        eval_conll(bi_nn, validation_tokens, validation_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002393920493691634"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тестирования сети мы подготовили для вас две функции:\n",
    " - *predict_tags*: получает батч данных и трансформирует его в список из токенов и предсказанных тегов;\n",
    " - *eval_conll*: вычисляет метрики precision, recall и F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_ner import precision_recall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, token_idxs_batch):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    tag_idxs_batch = model.predict_for_batch(token_idxs_batch)\n",
    "#     print(tag_idxs_batch)\n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
    "            if token_idx != token2idx['<PAD>']:\n",
    "#                 print(tag_idx)\n",
    "                tags.append(idx2tag[tag_idx])\n",
    "                tokens.append(idx2token[token_idx])\n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)\n",
    "    return tags_batch, tokens_batch\n",
    "    \n",
    "    \n",
    "def eval_conll(model, tokens, tags, short_report=True):\n",
    "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch in batches_generator(1, tokens, tags):\n",
    "        tags_batch, tokens_batch = predict_tags(model, x_batch)\n",
    "        ground_truth_tags = [idx2tag[tag_idx] for tag_idx in y_batch[0]]\n",
    "\n",
    "        # We extend every prediction and ground truth sequence with 'O' tag\n",
    "        # to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(tags_batch[0] + ['O'])\n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13258 tokens with 604 phrases; found: 983 phrases; correct: 176.\n",
      "\n",
      "precision:  17.90%; recall:  29.14%; F1:  22.18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    eval_conll(bi_nn, test_tokens, test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13258 tokens with 604 phrases; found: 788 phrases; correct: 182.\n",
      "\n",
      "precision:  23.10%; recall:  30.13%; F1:  26.15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    eval_conll(bi_nn, test_tokens, test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13258 tokens with 604 phrases; found: 739 phrases; correct: 153.\n",
      "\n",
      "precision:  20.70%; recall:  25.33%; F1:  22.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    eval_conll(bi_nn, test_tokens, test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперименты\n",
    "\n",
    "Задайте BiLSTMModel. Рекомендуемые параметры:\n",
    "- *batch_size*: 32;\n",
    "- начальное значение *learning_rate*: 0.01-0.001\n",
    "- *dropout_zeroed_probability*: 0.7-0.9\n",
    "- *embedding_dim*: 100-200\n",
    "- *rnn_hidden_size*: 150-200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1 мере на валидационной и тестовой выборках было не меньше 0.35. \n",
    "\n",
    "Если сеть плохо обучается, попробуйте использовать следующие модификации:\n",
    "    * используйте gradient clipping\n",
    "    * на каждой итерации уменьшайте learning rate (например, в 1.1 раз)\n",
    "    * попробуйте вместо Adam другие оптимизаторы \n",
    "    * экспериментируйте с dropout\n",
    "\n",
    "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "dropout_zeroed_probability = 0.8\n",
    "embedding_dim = 200\n",
    "rnn_hidden_size = 200\n",
    "\n",
    "bi_nn = BiLSTMModel(vocab_size, n_tags, PAD_index,\n",
    "                 embedding_dim, rnn_hidden_size,\n",
    "                 dropout_zeroed_probability)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bi_nn.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0325250ed37d499c828562e33065bb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 loss = tensor(0.2089, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 23 phrases; correct: 4.\n",
      "\n",
      "precision:  17.39%; recall:  0.66%; F1:  1.28\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 18 phrases; correct: 0.\n",
      "\n",
      "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
      "\n",
      "epoch = 1 loss = tensor(0.2106, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 22 phrases; correct: 4.\n",
      "\n",
      "precision:  18.18%; recall:  0.66%; F1:  1.28\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 39 phrases; correct: 0.\n",
      "\n",
      "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
      "\n",
      "epoch = 2 loss = tensor(0.2113, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 26 phrases; correct: 5.\n",
      "\n",
      "precision:  19.23%; recall:  0.83%; F1:  1.59\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 25 phrases; correct: 1.\n",
      "\n",
      "precision:  4.00%; recall:  0.19%; F1:  0.36\n",
      "\n",
      "epoch = 3 loss = tensor(0.2096, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 24 phrases; correct: 1.\n",
      "\n",
      "precision:  4.17%; recall:  0.17%; F1:  0.32\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 27 phrases; correct: 0.\n",
      "\n",
      "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
      "\n",
      "epoch = 4 loss = tensor(0.2128, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 26 phrases; correct: 5.\n",
      "\n",
      "precision:  19.23%; recall:  0.83%; F1:  1.59\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 25 phrases; correct: 2.\n",
      "\n",
      "precision:  8.00%; recall:  0.37%; F1:  0.71\n",
      "\n",
      "epoch = 5 loss = tensor(0.2088, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 23 phrases; correct: 5.\n",
      "\n",
      "precision:  21.74%; recall:  0.83%; F1:  1.59\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 29 phrases; correct: 3.\n",
      "\n",
      "precision:  10.34%; recall:  0.56%; F1:  1.06\n",
      "\n",
      "epoch = 6 loss = tensor(0.2098, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 31 phrases; correct: 2.\n",
      "\n",
      "precision:  6.45%; recall:  0.33%; F1:  0.63\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 34 phrases; correct: 0.\n",
      "\n",
      "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
      "\n",
      "epoch = 7 loss = tensor(0.2115, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 27 phrases; correct: 6.\n",
      "\n",
      "precision:  22.22%; recall:  0.99%; F1:  1.90\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 27 phrases; correct: 0.\n",
      "\n",
      "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
      "\n",
      "epoch = 8 loss = tensor(0.2093, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 20 phrases; correct: 3.\n",
      "\n",
      "precision:  15.00%; recall:  0.50%; F1:  0.96\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 30 phrases; correct: 2.\n",
      "\n",
      "precision:  6.67%; recall:  0.37%; F1:  0.71\n",
      "\n",
      "epoch = 9 loss = tensor(0.2094, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 26 phrases; correct: 3.\n",
      "\n",
      "precision:  11.54%; recall:  0.50%; F1:  0.95\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 29 phrases; correct: 2.\n",
      "\n",
      "precision:  6.90%; recall:  0.37%; F1:  0.71\n",
      "\n",
      "epoch = 10 loss = tensor(0.2109, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 20 phrases; correct: 0.\n",
      "\n",
      "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 31 phrases; correct: 2.\n",
      "\n",
      "precision:  6.45%; recall:  0.37%; F1:  0.70\n",
      "\n",
      "epoch = 11 loss = tensor(0.2093, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 23 phrases; correct: 5.\n",
      "\n",
      "precision:  21.74%; recall:  0.83%; F1:  1.59\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 32 phrases; correct: 2.\n",
      "\n",
      "precision:  6.25%; recall:  0.37%; F1:  0.70\n",
      "\n",
      "epoch = 12 loss = tensor(0.2084, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 23 phrases; correct: 5.\n",
      "\n",
      "precision:  21.74%; recall:  0.83%; F1:  1.59\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 29 phrases; correct: 1.\n",
      "\n",
      "precision:  3.45%; recall:  0.19%; F1:  0.35\n",
      "\n",
      "epoch = 13 loss = tensor(0.2114, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 29 phrases; correct: 6.\n",
      "\n",
      "precision:  20.69%; recall:  0.99%; F1:  1.90\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 33 phrases; correct: 3.\n",
      "\n",
      "precision:  9.09%; recall:  0.56%; F1:  1.05\n",
      "\n",
      "epoch = 14 loss = tensor(0.2120, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 26 phrases; correct: 2.\n",
      "\n",
      "precision:  7.69%; recall:  0.33%; F1:  0.63\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 27 phrases; correct: 3.\n",
      "\n",
      "precision:  11.11%; recall:  0.56%; F1:  1.06\n",
      "\n",
      "epoch = 15 loss = tensor(0.2092, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 31 phrases; correct: 3.\n",
      "\n",
      "precision:  9.68%; recall:  0.50%; F1:  0.94\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 27 phrases; correct: 1.\n",
      "\n",
      "precision:  3.70%; recall:  0.19%; F1:  0.35\n",
      "\n",
      "epoch = 16 loss = tensor(0.2098, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 26 phrases; correct: 5.\n",
      "\n",
      "precision:  19.23%; recall:  0.83%; F1:  1.59\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 40 phrases; correct: 2.\n",
      "\n",
      "precision:  5.00%; recall:  0.37%; F1:  0.69\n",
      "\n",
      "epoch = 17 loss = tensor(0.2117, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 26 phrases; correct: 6.\n",
      "\n",
      "precision:  23.08%; recall:  0.99%; F1:  1.90\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 28 phrases; correct: 1.\n",
      "\n",
      "precision:  3.57%; recall:  0.19%; F1:  0.35\n",
      "\n",
      "epoch = 18 loss = tensor(0.2094, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 25 phrases; correct: 3.\n",
      "\n",
      "precision:  12.00%; recall:  0.50%; F1:  0.95\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 27 phrases; correct: 5.\n",
      "\n",
      "precision:  18.52%; recall:  0.93%; F1:  1.77\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-310-5cf2f5bb824d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbi_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbi_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mbi_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPAD_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-293-36ab4efe4ea5>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x_batch, y_batch, optimizer, loss_function, clipping)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# update the parameters of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# number of epoch to train\n",
    "n_epoch = 20\n",
    "for epoch in tqdm_notebook(range(n_epoch)):\n",
    "    \n",
    "    losses = []\n",
    "    loss = 0\n",
    "    if epoch % 4 == 0:\n",
    "        learning_rate/=5\n",
    "    else:\n",
    "        learning_rate/= 1.1\n",
    "    optimizer = torch.optim.Adam(bi_nn.parameters(), lr=learning_rate)\n",
    "    for batch_num, (X_batch, y_batch) in enumerate(batches_generator(batch_size, train_tokens, train_tags)):\n",
    "        loss += bi_nn.train_on_batch(X_batch, y_batch, optimizer, loss_function)\n",
    "        bi_nn.embed.weight.data[PAD_index] = 0\n",
    "    losses.append(loss/batch_num)\n",
    "    with torch.no_grad():\n",
    "        print('epoch =',epoch,'loss =',losses[-1])\n",
    "        print('Test')\n",
    "        eval_conll(bi_nn, test_tokens, test_tags)\n",
    "        print('Validation')\n",
    "        eval_conll(bi_nn, validation_tokens, validation_tags)\n",
    "    nn.utils.clip_grad_norm_(bi_nn.parameters(), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 13258 tokens with 604 phrases; found: 27 phrases; correct: 5.\n",
      "\n",
      "precision:  18.52%; recall:  0.83%; F1:  1.58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    eval_conll(bi_nn, test_tokens, test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 1e-2\n",
    "dropout_zeroed_probability = 0.8\n",
    "embedding_dim = 200\n",
    "rnn_hidden_size = 150\n",
    "\n",
    "bi_nn = BiLSTMModel(vocab_size, n_tags, PAD_index,\n",
    "                 embedding_dim, rnn_hidden_size,\n",
    "                 dropout_zeroed_probability)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bi_nn.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fe04716a45497da3ce91ec4b21cc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 loss = tensor(0.2669, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 195 phrases; correct: 50.\n",
      "\n",
      "precision:  25.64%; recall:  8.28%; F1:  12.52\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 75 phrases; correct: 26.\n",
      "\n",
      "precision:  34.67%; recall:  4.84%; F1:  8.50\n",
      "\n",
      "epoch = 1 loss = tensor(0.1636, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 876 phrases; correct: 124.\n",
      "\n",
      "precision:  14.16%; recall:  20.53%; F1:  16.76\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 335 phrases; correct: 83.\n",
      "\n",
      "precision:  24.78%; recall:  15.46%; F1:  19.04\n",
      "\n",
      "epoch = 2 loss = tensor(0.1291, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1415 phrases; correct: 198.\n",
      "\n",
      "precision:  13.99%; recall:  32.78%; F1:  19.61\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 563 phrases; correct: 138.\n",
      "\n",
      "precision:  24.51%; recall:  25.70%; F1:  25.09\n",
      "\n",
      "epoch = 3 loss = tensor(0.1063, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1369 phrases; correct: 200.\n",
      "\n",
      "precision:  14.61%; recall:  33.11%; F1:  20.27\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 494 phrases; correct: 148.\n",
      "\n",
      "precision:  29.96%; recall:  27.56%; F1:  28.71\n",
      "\n",
      "epoch = 4 loss = tensor(0.0944, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1329 phrases; correct: 220.\n",
      "\n",
      "precision:  16.55%; recall:  36.42%; F1:  22.76\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 536 phrases; correct: 151.\n",
      "\n",
      "precision:  28.17%; recall:  28.12%; F1:  28.15\n",
      "\n",
      "epoch = 5 loss = tensor(0.0827, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1387 phrases; correct: 219.\n",
      "\n",
      "precision:  15.79%; recall:  36.26%; F1:  22.00\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 586 phrases; correct: 165.\n",
      "\n",
      "precision:  28.16%; recall:  30.73%; F1:  29.39\n",
      "\n",
      "epoch = 6 loss = tensor(0.0719, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1347 phrases; correct: 223.\n",
      "\n",
      "precision:  16.56%; recall:  36.92%; F1:  22.86\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 567 phrases; correct: 155.\n",
      "\n",
      "precision:  27.34%; recall:  28.86%; F1:  28.08\n",
      "\n",
      "epoch = 7 loss = tensor(0.0649, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1631 phrases; correct: 226.\n",
      "\n",
      "precision:  13.86%; recall:  37.42%; F1:  20.22\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 677 phrases; correct: 169.\n",
      "\n",
      "precision:  24.96%; recall:  31.47%; F1:  27.84\n",
      "\n",
      "epoch = 8 loss = tensor(0.0566, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1539 phrases; correct: 236.\n",
      "\n",
      "precision:  15.33%; recall:  39.07%; F1:  22.03\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 678 phrases; correct: 179.\n",
      "\n",
      "precision:  26.40%; recall:  33.33%; F1:  29.47\n",
      "\n",
      "epoch = 9 loss = tensor(0.0510, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1541 phrases; correct: 250.\n",
      "\n",
      "precision:  16.22%; recall:  41.39%; F1:  23.31\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 689 phrases; correct: 180.\n",
      "\n",
      "precision:  26.12%; recall:  33.52%; F1:  29.36\n",
      "\n",
      "epoch = 10 loss = tensor(0.0448, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1479 phrases; correct: 243.\n",
      "\n",
      "precision:  16.43%; recall:  40.23%; F1:  23.33\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 656 phrases; correct: 177.\n",
      "\n",
      "precision:  26.98%; recall:  32.96%; F1:  29.67\n",
      "\n",
      "epoch = 11 loss = tensor(0.0405, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1599 phrases; correct: 239.\n",
      "\n",
      "precision:  14.95%; recall:  39.57%; F1:  21.70\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 698 phrases; correct: 182.\n",
      "\n",
      "precision:  26.07%; recall:  33.89%; F1:  29.47\n",
      "\n",
      "epoch = 12 loss = tensor(0.0369, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1483 phrases; correct: 246.\n",
      "\n",
      "precision:  16.59%; recall:  40.73%; F1:  23.57\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 648 phrases; correct: 185.\n",
      "\n",
      "precision:  28.55%; recall:  34.45%; F1:  31.22\n",
      "\n",
      "epoch = 13 loss = tensor(0.0351, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1530 phrases; correct: 238.\n",
      "\n",
      "precision:  15.56%; recall:  39.40%; F1:  22.31\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 682 phrases; correct: 180.\n",
      "\n",
      "precision:  26.39%; recall:  33.52%; F1:  29.53\n",
      "\n",
      "epoch = 14 loss = tensor(0.0311, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1609 phrases; correct: 244.\n",
      "\n",
      "precision:  15.16%; recall:  40.40%; F1:  22.05\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 721 phrases; correct: 183.\n",
      "\n",
      "precision:  25.38%; recall:  34.08%; F1:  29.09\n",
      "\n",
      "epoch = 15 loss = tensor(0.0277, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1606 phrases; correct: 250.\n",
      "\n",
      "precision:  15.57%; recall:  41.39%; F1:  22.62\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 795 phrases; correct: 195.\n",
      "\n",
      "precision:  24.53%; recall:  36.31%; F1:  29.28\n",
      "\n",
      "epoch = 16 loss = tensor(0.0247, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1575 phrases; correct: 246.\n",
      "\n",
      "precision:  15.62%; recall:  40.73%; F1:  22.58\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 765 phrases; correct: 189.\n",
      "\n",
      "precision:  24.71%; recall:  35.20%; F1:  29.03\n",
      "\n",
      "epoch = 17 loss = tensor(0.0227, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1534 phrases; correct: 243.\n",
      "\n",
      "precision:  15.84%; recall:  40.23%; F1:  22.73\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 711 phrases; correct: 182.\n",
      "\n",
      "precision:  25.60%; recall:  33.89%; F1:  29.17\n",
      "\n",
      "epoch = 18 loss = tensor(0.0203, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1531 phrases; correct: 237.\n",
      "\n",
      "precision:  15.48%; recall:  39.24%; F1:  22.20\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 694 phrases; correct: 189.\n",
      "\n",
      "precision:  27.23%; recall:  35.20%; F1:  30.71\n",
      "\n",
      "epoch = 19 loss = tensor(0.0196, grad_fn=<DivBackward0>)\n",
      "Test\n",
      "processed 13258 tokens with 604 phrases; found: 1534 phrases; correct: 239.\n",
      "\n",
      "precision:  15.58%; recall:  39.57%; F1:  22.36\n",
      "\n",
      "Validation\n",
      "processed 12836 tokens with 537 phrases; found: 742 phrases; correct: 188.\n",
      "\n",
      "precision:  25.34%; recall:  35.01%; F1:  29.40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of epoch to train\n",
    "n_epoch = 20\n",
    "for epoch in tqdm_notebook(range(n_epoch)):\n",
    "    \n",
    "    losses = []\n",
    "    loss = 0\n",
    "    learning_rate/= 1.1\n",
    "    optimizer = torch.optim.Adam(bi_nn.parameters(), lr=learning_rate)\n",
    "    bi_nn.train(True)\n",
    "    for batch_num, (X_batch, y_batch) in enumerate(batches_generator(batch_size, train_tokens, train_tags)):\n",
    "        \n",
    "        loss += bi_nn.train_on_batch(X_batch, y_batch, optimizer, loss_function)\n",
    "#         bi_nn.embed.weight.data[PAD_index] = 0\n",
    "    losses.append(loss/batch_num)\n",
    "    \n",
    "    bi_nn.train(False)\n",
    "    with torch.no_grad():\n",
    "        print('epoch =',epoch,'loss =',losses[-1])\n",
    "        print('Test')\n",
    "        eval_conll(bi_nn, test_tokens, test_tags)\n",
    "        print('Validation')\n",
    "        eval_conll(bi_nn, validation_tokens, validation_tags)\n",
    "    nn.utils.clip_grad_norm_(bi_nn.parameters(), 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    eval_conll(bi_nn, test_tokens, test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [X] используйте gradient clipping (пробовал разные значения, ксожалению лучше не сделало, хотя пробовал в разных местах, мб неверно применяю, но посмотрел здесь поэтому по логике должно быть норм https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/main.py#L81)\n",
    " - [X] на каждой итерации уменьшайте learning rate (например, в 1.1 раз) (помогает улучшать минимум на 5 %)\n",
    " - [X] попробуйте вместо Adam другие оптимизаторы (Nesterov & AdaDelta не дали улучшенияк Adam)\n",
    " - [X] экспериментируйте с dropout (менял значения с 0.7-0.9, наилучшее 0.8)\n",
    " \n",
    "Как видим последняя модель, дала наилучшее качество на тесте > 30%.(best)\n",
    "\n",
    "К сожалению, она начинает переобучаться, что видно из колебаний значения F1 на test & validation).\n",
    "\n",
    "Архитектура очень чувствительна к настройке гиперпараметров, тк что немного отходит от best качество сразу проваливается. И увы только за 30 минут до конца делайна удалось установить, что lr =1e-2 дает лучше качество, тч не успел поделать тестов с ней.\n",
    "\n",
    "Также сделал собственоручное обнуление для embedding[PAD_index], взял с этого сайта https://discuss.pytorch.org/t/ignore-a-specific-index-of-embedding/12590"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
