{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem just right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "* The are two problems with \\* mark - they are not obligatory. You can get **EXTRA POINTS** for solving them.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Ridge Regression (1 point)\n",
    "Let us consider the problem of linear ridge regression for data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{d\\times 1}$. Let the objects have positive **sample weights** $v_{i}>0$, i.e. the optimization problem is\n",
    "$$\\sum_{i=1}^{n}v_{i}\\cdot L(y_{i}, \\hat{y}_{i})+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}=\\sum_{i=1}^{n}v_{i}\\cdot (\\boldsymbol{w}\\cdot\\boldsymbol{x}_{i}-y_{i})^{2}+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}\\rightarrow \\min_{\\boldsymbol{w}}.$$\n",
    "This problem reduces to classical linear ridge regression when $v_{i}\\equiv 1$. The matrix form of the problem is\n",
    "$$(Xw^{\\top}-y)^{\\top}V(Xw^{\\top}-y)+\\frac{\\lambda}{2}w^{\\top}w\\rightarrow\\min_{\\beta},$$\n",
    "where $V=V^{\\top}\\in\\mathbb{R}^{n\\times n}$ is the diagonal matrix with diagonal elements $v_{1},\\dots, v_{n}$. Note that the quadratic problem is still convex (w.r.t. $\\boldsymbol{w}$), thus, the solution is unique. Solve this problem for any positive-defined matrix $V$ (not just diagonal) and provide the answer in the matrix form.\n",
    "### Your solution:\n",
    "$$(Xw^{\\top}-y)^{\\top}V(Xw^{\\top}-y)+\\frac{\\lambda}{2}w^{\\top}w\\rightarrow\\min_{\\beta},$$\n",
    "\n",
    "<center>$$\\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} =~\\frac{\\partial}{\\partial \\vec{w}}[(Xw^{\\top}-y)^{\\top}V(Xw^{\\top}-y)+\\frac{\\lambda}{2}w^{\\top}w]~=~\\frac{\\partial}{\\partial \\vec{w}}[w^TX^TVXw-2y^TXVw-y^TVY+\\frac{\\lambda}{2}w^Tw] ~=~2X^{\\top}VXw - 2X^{\\top}Vy+\\lambda w$$ </center>\n",
    "\n",
    "\n",
    "<center> $$ \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} = 0 \\Leftrightarrow X^{\\top}VXw - X^{\\top}Vy+\\frac{\\lambda}{2}w = 0$$ $$  X^{\\top}VXw+\\frac{\\lambda}{2}w= X^{\\top}Vy $$ $$w ~=~ (X^{\\top}VX+\\frac{\\lambda}{2})^{-1}X^{\\top}Vy$$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Logistic Regression (1 point)\n",
    "Let us consider the case when in the problem of binary classification the training set is lineary separable. Show that in this case the optimization problem for logistic regression **without L2-regularization** does not have optimum.\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let  for input space $X$, $x_1$ is class 1 and $x_{-1}$ is class -1. \n",
    "$$\\langle a,x_{-1} \\rangle < 0 < \\langle a,x_1 \\rangle$$\n",
    "\n",
    "For labels, $y_1$ = 1, $y_{-1}$ = -1\n",
    "\n",
    "As training set is lineary separable, then we can define condition for logistic regresison :\n",
    "\n",
    "$$ \\langle a,xi,yi \\rangle > 0 ~~ for~ all~ xi \\in X$$\n",
    "\n",
    "And functional for logistic regression:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^{n} log(1+exp(-(w^{T}x_i)y_i))\\rightarrow \\min_{\\boldsymbol{w}}$$\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}}= \\sum_{i=1}^{n} -x_i y_i \\frac{exp(-(w^{T}x_i)y_i))}{1+exp(-(w^{T}x_i)y_i))}$$\n",
    "$$\\langle \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}}, a \\rangle = \\sum_{i=1}^{n}\\frac{exp(-(w^{T}x_i)y_i))}{1+exp(-(w^{T}x_i)y_i))} \\langle a, -x_i y_i \\rangle < 0$$\n",
    "\n",
    "and we can see that it doesn't satisfy our condition because\n",
    "$$ \\frac{exp(-(w^{T}x_i)y_i))}{1+exp(-(w^{T}x_i)y_i))} > 0~and~ \\langle a, -x_i y_i \\rangle < 0$$\n",
    "\n",
    "then  logistic regression without L2-regularization does not have optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Bayesian Naive Classifier (1 point)\n",
    "Show that in case of $d$ binary-valued features $x_{j}\\in\\{0, 1\\}$ (for $j=1,2,\\dots,d$) Bayesian Naive Classifier's decision rule is linear.\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Naive Classifier for binary-valued features:\n",
    "$$ p(X|C_k) = \\prod_{j=1}^{d}p_{k_i}^{x_i}(1 − p_{ki})^{(1−x_i)}$$\n",
    "\n",
    "In log-scale we get linear classifier\n",
    "\n",
    "$$L = log~ p(C_k) + \\sum_{j=1}^{d} x_i~log~p_{k_i} + (1-x_i)~log~(1-p_{k_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum a posteriori (MAP) decision rule:\n",
    "$$ \\hat{y} = argmax[log~ p(C_k) + \\sum_{j=1}^{d} x_i~log~p_{k_i} + (1-x_i)~log~(1-p_{k_i})] $$ Simplify it:\n",
    "$$ \\sum_{j=1}^{d} x_i~log~p_{k_i} + (1-x_i)~log~(1-p_{k_i}) \\Leftrightarrow $$ $$\\sum_{j=1}^{d} x_i~log~p_{k_i} + -x_i~log~(1-p_{k_i}) +log~(1-p_{k_i}) = \\sum_{j=1}^{d} x_i~log~\\frac{p_{k_i}}{1-p_{k_i}}+log~(1-p_{k_i})  $$\n",
    "\n",
    "Consequently, $$ \\hat{y} = argmax[w^Tx+b]$$\n",
    "where $$b = ~log~ p(C_k) + \\sum_{j=1}^{d}log~(1-p_{k_i})~~;~~ w_i^T = log~\\frac{p_{k_i}}{1-p_{k_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "$$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\bigg[\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg)\\bigg],$$\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write the error rate in the cross-validation functional through the sum of the error indicators and change the summation signs:\n",
    "\n",
    "$$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\bigg[\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg)\\bigg]$$\n",
    "\n",
    "Suppose we know $m_i$(the number of closest neighbours which is contained in $X^{n}\\ X'$)for every $y_i \\in X'$, then we have \n",
    "\n",
    "$$\n",
    "L_{k}OCV=\\frac{1}{C_{n}^{k}}\\bigg[\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq y_i^{m_i}]\\bigg)\\bigg]\n",
    "$$\n",
    "\n",
    "Now  calculate how many subsets of $X^n$ have $m-1$ closet neighbors. So it is $m-k-1$ of elements, who is not in $X'$ from n-m-1(we know that $m-th$ closest neighbours not in $X'$). The number of such partitions is exactly equal to C_{n-m-1}^{n-k-1} because we should take $n-k-1$ elements for \n",
    "\n",
    "so,\n",
    "\n",
    "The first m-1 objects fall into the control subsample, the m-th neighbor is in the training subsample and belongs to another class.  The number of such partitions is exactly equal to $C_{n-m-1}^{n-k-1}$ because we should take $n-k-1$ elements for control subsample and  $n-m-1$ in another subsample( $m$ closest neighbours and $y$)\n",
    "$$L_{k}OCV=\\sum_{i=1}^{n}\\frac{1}{C_n^kk}\\sum_{m=1}^{k}C_{n-m-1}^{n-k-1}[y_i\\neq y_i^m]\n",
    "$$\n",
    "\n",
    "Take number and transform equation\n",
    "\n",
    "$$\n",
    "\\sum_{m=1}^{k}\\frac{C_{n-m-1}^{n-k-1}}{C_n^kK}\\sum_{i=1}^{n}[y_i\\neq y_i^m]=\\sum_{m=1}^{k}\\frac{1}{n}\\sum_{i=1}^{n}[y_i\\neq y_i^m]\\frac{C_{n-k-1}^{n-1-m}}{C_{n-1}^{k-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability of object $x_{i}$ to be present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "\n",
    "### Your solution:\n",
    "\n",
    "The probability of object $x_{i} \\in \\hat{X}^{n}$:\n",
    "$$P(x_{i} \\in \\hat{X}^{n}) = 1 - \\bigg (1- \\frac{1}{n} \\bigg)^n$$\n",
    "\n",
    "Let compute the limit of this probability for $n\\rightarrow\\infty$: \n",
    "$$ \\lim\\limits_{n\\rightarrow \\infty} P(x_{i} \\in \\hat{X}^{n}) = 1 - e^{-1} \\approx 0.632$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1+1=3 points)\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample, i.e. $\\frac{1}{n}\\sum_{i=1}^{n}L(y_{i}, \\hat{y})\\rightarrow\\min$. We consider three cases:\n",
    "1. Regression tree ($y_{i}\\in\\mathbb{R}$), absolute loss function $L(y,\\hat{y})=|y-\\hat{y}|$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-\\hat{y}|$ is the median of labels: \n",
    "$$\\hat{y}=\\text{median}(y_{1},\\dots,y_{n}).$$\n",
    "In this case, for simplicity you may assume that $n$ is even (or odd, as you wish).\n",
    "2. Regression tree ($y_{i}\\in\\mathbb{R}$), squared loss function $L(y,\\hat{y})=(y-\\hat{y})^{2}$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y})^{2}$ is the mean of labels:\n",
    "$$\\hat{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i.$$\n",
    "3. Classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$), zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq\\hat{y}]$ is the most frequent label:\n",
    "$$\\hat{y}=\\underset{k=1,2,\\dots,K}{\\operatorname{argmax}}\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=k].$$\n",
    "In this case, for simplicity you may assume that there is only one most frequent label.\n",
    "\n",
    "Suppose that that instead of using optimal prediction for this leaf we output the label from $y_{1},\\dots,y_{n}$ at random. What will happen with the (expected) loss on the training sample (will it increase/decrease/not change)? Prove your answer (separately for every case).\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6.1 As $\\hat{y}=\\text{median}(y_{1},\\dots,y_{n}).$ then 50% of y located under median and half above. Then if we will have random labels, it will increase loss functional because it include dirty values () which influence on loss function.\n",
    "\n",
    "- 6.2\n",
    "The optimal prediction is  $\\hat{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i$  for $L(y,\\hat{y})=(y-\\hat{y})^2$.\n",
    "\n",
    "Also, we know that our function is convex(second derivative non-negative for all values), that is mean that predictation have only one optium value(mimnimize functional). I can conclude, that if we take  random point, expected loss will increased.\n",
    "\n",
    "$$E(L(y))-L(\\hat{y})=\\sum_{i=1}^{m}p(\\hat{L}y_i-\\hat{L}\\hat{y})$$\n",
    "$$so~~ for~~ y \\neq \\hat{y} ~then~ \\sum_{i=1}^{m}p(\\hat{L}y_i-\\hat{L}\\hat{y})>0$$\n",
    " $$ if ~y~  = \\hat{y} ~then~\\sum_{i=1}^{m}p(\\hat{L}y_i-\\hat{L}\\hat{y}) = 0$$\n",
    " \n",
    " - 6.3\n",
    " Let random output is $b$.\n",
    " Look on expected value of loss function: $E[L(y,\\hat{y})]=\\frac{1}{n}\\sum_{i=1}^{n}[y_i\\neq \\hat{y}]$ ,as we have optimal solution\n",
    " $\\hat{y}=\\underset{k=1,2,\\dots,K}{\\operatorname{argmax}}\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=k].$ then:\n",
    " $$E[L(y,\\hat{y})]=\\frac{1}{n}\\sum_{i=1}^{n}[y_i\\neq b] \\geq \\frac{1}{n}\\sum_{i=1}^{n}[y\\neq \\hat{y}] $$\n",
    " In any case loss will increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7*. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
    "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
    "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$.\n",
    "### Your solution:\n",
    "#### 1 Prove (from MachineLearning.ru)\n",
    "we can represent $f$ classifier as\n",
    "$$\n",
    "y=sign(f(x)-w_0)\n",
    "$$\n",
    "which vary $w_0$ for prediction \n",
    "\n",
    "Lets \n",
    "\n",
    "$$\n",
    "R=\\sum[y=1]~~and~~W=\\sum[y=0] \n",
    "$$\n",
    "\n",
    "The ROC curve shows the dependence of TPR on FPR when the threshold $w_0$ is varied.\n",
    "\n",
    "If $w_0>\\max_i(f(x_i))$ then all objects are wrong(label = 0) and $TPR=0$, $FPR=0$.\n",
    "\n",
    "If $w_0<\\max_i(f(x_i))$ then all objects are right(label = 1) and $TPR=1$, $FPR=1$.\n",
    "\n",
    "We need $w_0>\\max(f(x_i))$\n",
    "Objects will intersect the $w_0$ and\n",
    "\n",
    "if $y_i=0$, $FPR$ will increase by $\\frac{1}{W}$, $AUC$ increased by $\\frac{TPR}{N}$\n",
    "In case, that $y_i=1$, $TPR$ will increase by $\\frac{1}{R}$ \n",
    "\n",
    "Finally: $AUC = \\frac{1}{WR}\\sum_{i<j}[y_j<y_i]$\n",
    "\n",
    "#### 2 Prove(from Wikipedia)\n",
    "From [wiki1](https://ru.wikipedia.org/wiki/ROC-%D0%BA%D1%80%D0%B8%D0%B2%D0%B0%D1%8F) and [wiki2](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "\n",
    "we know that Area under the ROC is equal to:\n",
    " $$ A = \\int_{\\infty}^{-\\infty} TPR(T) FPR^{'}(T)dT = \\langle TPR \\rangle ~~~ -~mean(TPR) $$\n",
    " \n",
    " TPR is measures the proportion of actual positives that are correctly identified.\n",
    " \n",
    " Then $\\frac{1}{n_1(n-n_1)}$ will be count of all neccesary pairs and classification $\\sum_{i<j}[y_{i}<y_{j}]$ take more probabilities that object being from class $1$ i.e. take s the proportion of actual positives that are correctly identified. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernel Regression (1 point)\n",
    "Recall that the prediction of Kernel Ridge Regression fitted on data $X^{n}$ with the kernel $K(\\cdot, \\cdot)$ has the form $\\mathcal{K}(x)=\\sum_{i=1}^{n}\\alpha_{i}K(x, x_{i})$, where $\\alpha=(K+\\lambda I)^{-1}Y$ ($K_{ij}=K(x_{i},x_{j})$). The time complexity of computation of a prediction $\\mathcal{K}(x)$ for any point $x$ is $O(n)$, i.e. grows linearly with the size of the training set.\n",
    "\n",
    "Consider the bilinear kernel $K(x, x')=\\langle x, x'\\rangle$. For this kernel, the Kernel Regression is known to turn into simple linear ridge regression. However, for linear regression the computation time of prediction $\\mathcal{R}(x)=\\sum_{j=1}^{d}\\beta_{j}x^{j}$ is $O(d)$, where $d$ is the dimension of the feature space and does not grow with the training, which is a little bit controversary to the previous paragraph.\n",
    "\n",
    "In this task in order to show that the kernel regression with the bilinear kernel is indeed the linear ridge regression, you have to prove that the predictions exactly match by **direct comparison**.\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bilinear kernel $K(x, x')=\\langle x, x'\\rangle$ Kernel Ridge Regression is:\n",
    "\n",
    "$$\n",
    "\\hat{f}(x)= \\sum_{i=1}^{m}\\alpha_iK(x_i, x)=<by~Kernel ~definition >=\\sum_{i=1}^{m}\\alpha_i(\\Phi(X_i) \\Phi(X)^T~~~,where\n",
    "$$ \n",
    "$$\n",
    "\\alpha=(\\Phi(X) \\Phi(X)^T+\\lambda I)^{-1}Y=(K+\\lambda I)^{-1}Y,~~K_m = \\{\\Phi(X_i) \\Phi(X_j)^T\\}_{i,j=1}^m = {K(x_i,x_j)}\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\hat{f}(x)=\\sum_{i=1}^{m}\\alpha_i \\langle x_i, x \\rangle=XX^T(XX^T+\\lambda I)^{-1}Y\n",
    "$$\n",
    "\n",
    "Let is make **direct comprasion**, for Ridge Regression:\n",
    "$$\n",
    "F(w)=||XW-Y||^2+\\lambda||w||^2\n",
    "$$\n",
    "where \n",
    "$$\n",
    "W=(X^TX+\\lambda I)^{-1}X^TY=X^T(XX^T+\\lambda I)^{-1}Y\n",
    "$$\n",
    "\n",
    "we can represent as $$W=X^T\\alpha $$\n",
    "\n",
    "and \n",
    "$$\n",
    "\\hat{f}(x)=XW=\\sum_{i=1}^{m}\\alpha_i\\langle x, x_i\\rangle=XX^T(XX^T+\\lambda I)^{-1}Y\n",
    "$$\n",
    "they are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that Gaussian Kernel $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite.\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)Prove (from Expected value)\n",
    "From definition of positive definite kernel:\n",
    "\n",
    " 1. $K(x,x') = K(x',x)$ is symmetric\n",
    " 2. let $K(x,x') = \\phi(x-x')$, where $ \\phi(t) = exp(- \\frac{t^2}{2}) = M[exp(i~t~Z)]$ and Z is a random variable from Normal distribution $N(0,1)$. \n",
    " \n",
    " And $y_1 \\dots y_n$ & $c_1 \\dots c_n ~ \\in \\mathbb{R}$\n",
    " $$\\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\phi(y_i-y_j) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j M[e^{(i~(y_i-y_j)~Z}] = M[\\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i e^{i y_i Z} c_j e^{-i y_j Z} = M[|\\sum_{i=1}^{n} c_i e^{i y_i Z}|^2] \\geq 0~(from~ property~of~Expected~Value)$$\n",
    " \n",
    "#### 2) Prove (by induction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from fedinition of positive definite kernel we remember that it should be symmetric and \n",
    "$$\\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j K(x_i,x_j) \\geq 0$$\n",
    "\n",
    "Remember that $\\exp(-\\|x-x'\\|^{2}) \\in [0,1]$ then problem can be in constants.\n",
    "\n",
    "for any $x_1 \\dots x_n \\in \\mathbb{X}$ and $c_1 \\dots c_n \\in \\mathbb{C}$ (because the number multiplied by the conjugate is non-negative) look at this Kernel in different dimension:\n",
    "\n",
    " \n",
    " - n = 1 \n",
    " $$ c_1 c_1 \\exp(-\\|x_1-x_1\\|^{2}) = c_1^2 \\geq 0$$\n",
    " \n",
    " - n = 2 \n",
    " $$ c_1^2 +2 c_1 c_2 \\exp(-\\|x_1-x_2\\|^{2}) + c_2^2 \\geq (c_1-c_2)^2 \\geq 0 $$\n",
    " - n = 3 \n",
    " $$\\sum_{i=1}^{3} \\sum_{j=1}^{3} c_i c_j \\exp(-\\|x_i-x_j\\|^{2}) \\geq  (c1-c2-c3)^2 \\geq 0$$\n",
    " \n",
    " - and finally we can conclude that in common case \n",
    " \n",
    " $$\\sum_{i=1}^{3} \\sum_{j=1}^{3} c_i c_j \\exp(-\\|x_i-x_j\\|^{2})\\in \\bigg[0,\\bigg( \\sum_{j=0}^{n} c_j\\bigg) ^2 \\bigg)$$\n",
    " \n",
    " As we can see ,this kernel is positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10*. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
