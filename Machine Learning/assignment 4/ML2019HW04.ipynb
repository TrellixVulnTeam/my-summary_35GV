{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 4 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write your implementation within the designated blocks:\n",
    "```python\n",
    "...\n",
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "\n",
    "### END Solution\n",
    "...\n",
    "```\n",
    "\n",
    "Write your theoretical derivations within such blocks:\n",
    "```markdown\n",
    "**BEGIN Solution**\n",
    "\n",
    "<!-- >>> your derivation here <<< -->\n",
    "\n",
    "**END Solution**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly / outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will need to work through a modified version of\n",
    "the SVDD model (**support vector data description**) -- a model for outlier\n",
    "detection, and show some theoretical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have a dataset $x_1, \\ldots, x_l$ from some set $\\mathcal{X}$.\n",
    "\n",
    "We apply the kernel trick using the kernel $K \\colon \\mathcal{X} \\times \\mathcal{X}\n",
    "\\to \\mathbb{R}$ of the Hilbert space $\\bigl(\\mathcal{H}, \\langle \\cdot,\n",
    "\\cdot \\rangle\\big)$ with the feature mapping $\\phi(\\cdot)\\colon \\mathcal{X}\n",
    "\\to \\mathcal{H}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that $\\nu \\in (0, 1)$, and $C_i > 0$ with $\\sum_{i=1}^l C_i = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVDD model (kernelized) is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\underset{R, h, \\xi}{\\text{minimize}}\n",
    "      & & R + \\frac1\\nu \\sum_{i=1}^l C_i \\xi_i\n",
    "          \\,, \\\\\n",
    "    & \\text{subject to}\n",
    "      & & \\|\\phi(x_i) - h \\|^2 \\leq R + \\xi_i\n",
    "          \\,, \\\\\n",
    "    & & & \\xi_i \\geq 0\n",
    "          \\,.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use next papers for tasks(1-5):\n",
    "\n",
    " 1. [A Revisit to Support Vector Data Description](https://www.csie.ntu.edu.tw/~cjlin/papers/svdd.pdf)\n",
    " 2. [Support Vector Data Description](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.1425&rep=rep1&type=pdf)\n",
    " 3. [Одноклассовая машина опорных векторов с использованием\n",
    "привилегированной информации](http://itas2016.iitp.ru/pdf/1570285426.pdf)\n",
    " 4. https://arxiv.org/pdf/1609.08039.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (3 pt.): Can $R$ be negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the SVDD problem with an extra constraint $R \\geq 0$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\underset{R, h, \\xi}{\\text{minimize}}\n",
    "      & & R + \\frac1\\nu \\sum_{i=1}^l C_i \\xi_i\n",
    "          \\,, \\\\\n",
    "    & \\text{subject to}\n",
    "      & & \\|\\phi(x_i) - h \\|^2 \\leq R + \\xi_i\n",
    "          \\,, \\\\\n",
    "    & & & \\xi_i \\geq 0\n",
    "          \\,, \\\\\n",
    "    & & & R \\geq 0\n",
    "          \\,.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R \\geq 0$ constraint is a nuisance.\n",
    "\n",
    "* Show, that if $(R, \\xi, h)$ has $R < 0$, then it **is not better** than a\n",
    "certain other feasible candidate $(\\hat{R}, \\hat{\\xi}, \\hat{h})$ with $\\hat{R} \\geq 0$.\n",
    "* Argue that it is, in fact, **redundant**, i.e. the problem formulations\n",
    "**with it** and **without it** have identical solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Let's consider a new point (0, h, $\\xi+Re$), where $e$ is vector of ones. It's feasible because\n",
    "$$ 0 \\leq \\|\\phi(x_i) - h \\|^2 \\leq R + \\xi_i = 0 + (\\xi_i+R) \\to \\xi_i+R \\geq 0$$\n",
    "\n",
    "As $\\nu  \\leq l$ & $ R < 0$, we can rewrite our problem \n",
    "\n",
    "$$ 0+\\frac1\\nu \\sum_{i=1}^l C_i (\\xi_i + R) = \\frac R \\nu \\sum_{i=1}^l C_i +\\frac1\\nu \\sum_{i=1}^l C_i \\xi_i =$$ $$\\frac R\\nu +\\frac1\\nu \\sum_{i=1}^l C_i \\xi_i < \\frac1\\nu \\sum_{i=1}^l C_i \\xi_i + R$$\n",
    "\n",
    "a contradiction to the assumption that ($R,a,\\xi$) is optimal.  \n",
    "\n",
    "Then we can find a certain other feasible candidate $(\\hat{R}, \\hat{\\xi}, \\hat{h})$ with $\\hat{R} \\geq 0$\n",
    "\n",
    " * Therefore, even if the $R \\geq 0$ constraint is not explicitly stated in our problem , it is still satisfied by any optimal solution <br>\n",
    " [B Proof of Theorem 4](https://www.csie.ntu.edu.tw/~cjlin/papers/svdd.pdf)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (2 pt.): Can $\\xi_i > 0$ for all $i$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argue if $(R, \\xi, h)$ is a solution, then $\\xi_i = 0$ for at least one $i=1,\\,\\ldots,\\,l$. Let $\\bar{\\xi} = \\min_{j=1}^l \\xi_j$.\n",
    "\n",
    "**HINT** Use an argument similar to Task $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As $0 \\leq \\|\\phi(x_i) - h \\|^2 \\leq R + \\xi_i$, $\\bar{\\xi} = \\min_{j=1}^l \\xi_j$ & let $\\bar{\\xi} \\ne 0$ then:\n",
    "$$ \\hat{R} = R + \\bar{\\xi} \\geq 0 \\\\ \\hat{\\xi_i} = \\xi - \\bar{\\xi} \\geq 0$$\n",
    "\n",
    "Consider problem from 1 task:\n",
    "\n",
    "$$ \\hat{R}+ \\frac1\\nu \\sum_{i=1}^l C_i  \\hat{\\xi_i} = R +\\frac1\\nu \\sum_{i=1}^l C_i \\xi_i +\\bar{\\xi}(1 - \\frac1\\nu \\sum_{i=1}^l C_i )= \\\\ = R +\\frac1\\nu \\sum_{i=1}^l C_i \\xi_i +\\bar{\\xi}(1 -\\frac1\\nu ) \\leq \\left<as~ \\bar{\\xi}(1 -\\frac1\\nu )< 0 \\right> \\leq R + \\frac1\\nu \\sum_{i=1}^l C_i \\xi_i $$\n",
    "\n",
    "We can make better our minimization task with  $\\hat{R}$ and $\\hat{\\xi_i}$, where can be $\\bar{\\xi} = 0$ and it is optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (2 pt.): The Lagrangian and KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write out the Lagrangian function of the problem and write out the KKT conditions\n",
    "* Lagrangian\n",
    "* the first order conditions\n",
    "* the complementary slackness conditions\n",
    "* the primal and dual constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " * $$ L = R + \\frac1\\nu \\sum_{i=1}^l C_i \\xi_i + \\sum_{i=1}^l \\alpha_i \\left( \\|\\phi(x_i) - h\\|^2 - R - \\xi_i \\right) - \\sum_{i=1}^l\\beta_i\\xi_i \\\\ \\alpha_i \\geq 0 ~~ \\beta_i \\geq 0 ~~$$ (look without $R > 0$) \n",
    " \n",
    " * $$\n",
    "\\frac{\\partial L}{\\partial R} = 1 - \\sum_{i=1}^l \\alpha_i = 0 \\to \\sum_{i=1}^l \\alpha_i = 1\n",
    "$$\n",
    "\n",
    " $$\n",
    "\\frac{\\partial L}{\\partial h} = -2\\sum_{i=1}^l \\alpha_i \\left(\\phi(x_i)-h \\right)= 0\n",
    "$$\n",
    "\n",
    " $$\n",
    "\\frac{\\partial L}{\\partial \\xi_i} = \\frac1\\nu C_i - \\alpha_i - \\beta_i  = 0\n",
    "$$\n",
    "\n",
    " * the **complementary slackness** conditions\n",
    " $  \\alpha_i \\left( \\|\\phi(x_i) - h\\|^2 - R - \\xi_i \\right) = 0 $\n",
    " \n",
    " $\\beta_i \\xi_i = 0 $\n",
    " \n",
    " * **Primal** constraints  $$ \\|\\phi(x_i) - h\\|^2 - R - \\xi_i  \\leq 0 $$\n",
    " $$ \\xi_i \\geq 0$$\n",
    " \n",
    " \n",
    " **Dual** constraints\n",
    " $$\\alpha_i \\geq 0 ~~ \\beta_i \\geq 0$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (2 pt.): Simplifying the Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down $h$ as a function of transformed input data and dual coefficients,\n",
    "and using the first order conditions rewrite the Lagrangian in such a way, that\n",
    "it only contains dual variables and evaluations of the kernel $K(\\cdot, \\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* We use FOCs from task 3\n",
    "$$\n",
    "\\sum_{i=1}^l \\alpha_i \\left(h - \\phi(x_i) \\right)= \\sum_{i=1}^l \\alpha_i h - \\sum_{i=1}^l \\alpha_i \\phi(x_i) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "h \\sum_{i=1}^l \\alpha_i = \\sum_{i=1}^l \\alpha_i \\phi(x_i) \\to h = \\frac{\\sum_{i=1}^l \\alpha_i \\phi(x_i)}{\\sum_{i=1}^l \\alpha_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^l \\alpha_i = 1 \\to h= \\sum_{i=1}^l \\alpha_i \\phi(x_i)\n",
    "$$\n",
    "\n",
    "* $$L = R + \\frac1\\nu \\sum_{i=1}^l C_i \\xi_i + \\sum_{i=1}^l \\alpha_i \\left( \\|\\phi(x_i) - h\\|^2 - R - \\xi_i \\right) - \\sum_{i=1}^l\\beta_i\\xi_i =$$ \n",
    "$$ R \\left( 1 - \\sum_{i=1}^l\\alpha_i \\right)+ \\sum_{i=1}^l \\xi_i \\left( \\frac1\\nu C_i - \\alpha_i - \\beta_i \\right)+\\sum_{i=1}^l \\alpha_i \\left( \\|\\phi(x_i) - h \\|^2  \\right) \\to$$\n",
    "$$\\left< \\text{from equallity FOCs to zero} \\right>~~L = \\sum_{i=1}^l \\alpha_i \\left( \\|\\phi(x_i) - h \\|^2  \\right) $$\n",
    "then\n",
    "$$\n",
    "    L = \\sum_{i=1}^l \\alpha_i \\| \\phi(x_i) - \\sum_{j=1}^l \\alpha_j \\phi(x_j) \\| ^2\n",
    "        = \\sum_{i=1}^l  \\alpha_i < \\phi(x_i) - \\sum_{j=1}^l \\alpha_j \\phi(x_j) , \\phi(x_i) - \\sum_{j=1}^l \\alpha_j \\phi(x_j) > \n",
    "        \\\\\n",
    "        = \\sum_{i=1}^l  \\alpha_i \\left( K(x_i,x_i) - 2 \\sum_{j=1}^l \\alpha_j K(x_i, x_j) + \\sum_{j=1}^l \\sum_{k=1}^l \\alpha_j \\alpha_k K (x_j, x_k) \\right)\n",
    "        \\\\\n",
    "        = \\sum_{i=1}^l  \\alpha_i K(x_i,x_i) - 2  \\sum_{i=1}^l  \\alpha_i \\sum_{j=1}^l \\alpha_j K(x_i, x_j) + \\sum_{i=1}^l  \\alpha_i \\sum_{j=1}^l \\sum_{k=1}^l \\alpha_j \\alpha_k K (x_j, x_k)  \n",
    "        \\\\\n",
    "        = \\left< as \\sum_{i=1}^l  \\alpha_i = 1 \\text{ where K don't depend from index i} \\right>\n",
    "        = \\sum_{i=1}^l  \\alpha_i K(x_i,x_i)  - \\sum_{j=1}^l \\sum_{k=1}^l \\alpha_j \\alpha_k K (x_j, x_k) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 (2 pt.): The dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Carefully eliminate $\\beta_i$ from the KKT conditions, and write\n",
    "down the inequality constraints for the dual variables $\\alpha_i$,\n",
    "implied by the FOC.\n",
    "\n",
    "* Combine your results into dual problem (minimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "From ~~\\frac{\\partial L}{\\partial \\xi_i} = \\frac1\\nu C_i - \\alpha_i - \\beta_i  = 0 \\to  \\beta_i = \\frac1\\nu C_i- \\alpha_i$$ $$as~~ \\beta_i \\geq 0 ~then ~~\\alpha_i \\leq \\frac1\\nu C_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^l  \\alpha_i K(x_i,x_i)  - \\sum_{i=1}^l \\sum_{j=1}^l \\alpha_i \\alpha_j K (x_i, x_j) \\to \\underset{\\alpha}{max} \\\\s.t. 0 \\leq \\alpha_i \\leq \\frac1\\nu C_i ,\\\\  \\sum_{i=1}^l \\alpha_i = 1\n",
    "$$\n",
    "\n",
    "[link](http://itas2016.iitp.ru/pdf/1570285426.pdf)\n",
    "\n",
    "for minimization you can multiply on (-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 (2 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, you have solved the dual and have optimal $(\\alpha^*_i)_{i=1}^l$ and\n",
    "$ h^* = \\sum_{i=1}^l \\alpha^*_i \\phi(x_i)$.\n",
    "\n",
    "Let $M = \\{i\\colon \\alpha^*_i \\in \\left(0, \\tfrac{C_i}{\\nu}\\right)\\}$ and suppose $M \\neq \\emptyset$.\n",
    "\n",
    "* Derive the expression for the optimal value of $R$ from this and the\n",
    "complementary sclackness conditions.\n",
    "\n",
    "* Write out the decision function for an arbitrary $x\\in \\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " * the complementary slackness conditions\n",
    " \n",
    " $  \\alpha_i \\left( \\|\\phi(x_i) - h^*\\|^2 - R - \\xi_i \\right) = 0 \\to \\|\\phi(x_i) - h^*\\|^2 - R - \\xi_i = 0$\n",
    "  $$\\beta_i \\xi_i = 0 ~~as~~ \\alpha^*_i \\in \\left(0, \\tfrac{C_i}{\\nu}\\right)~~then~~ \\xi_i = 0$$\n",
    "$$ R = \\|\\phi(x_i) - h^*\\|^2 = \\|\\phi(x_i)\\|^2 - 2 (h^* \\cdot \\phi(x_i)^T)+ \\|\\ h^*\\|^2  = K(x, x) - 2\\sum_{i=1}^l \\alpha_i K(x_i,x)+ \\sum_{j=1}^l \\alpha_i \\alpha_j K (x_i, x_j)$$\n",
    " * the decision function for an arbitrary $x\\in \\mathcal{X}$ \n",
    " $$ f(x)~=~sign\\left(R-K(x,x)+2\\sum_{i=1}^l \\alpha_i K(x,x_i)-|| h^*||^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.1 (2 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that for some $x_i$ we have $\\|\\phi(x_i) - h\\|^2 < R$.\n",
    "We will call this point **inlier**.\n",
    "\n",
    "* What are the values of $\\alpha_i$ and $\\beta_i$ for such a point?\n",
    "* Show that $1-\\nu$ upper-bounds the sum of weights $C_i$ for the **inlier** points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " * As for some $x_i$ we have $\\|\\phi(x_i) - h\\|^2 < R$.Then for the complementary slackness conditions\n",
    " $  \\alpha_i \\left( \\|\\phi(x_i) - h\\|^2 - R - \\xi_i \\right) = 0 \\to \\alpha_i = 0$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\xi_i} = \\frac1\\nu C_i - \\alpha_i - \\beta_i  = 0 \\to \\beta_i = \\frac1\\nu C_i\n",
    "$$\n",
    "\n",
    " * Let's look at their sum:\n",
    " $$\\sum_{i=1}^l \\frac1\\nu C_i - \\sum_{i=1}^l \\alpha_i - \\sum_{i=1}^l \\beta_i  = 0 \\to \\sum_{i=1}^l \\beta_i = \\frac1\\nu \\sum_{i=1}^l C_i - \\sum_{i=1}^l \\alpha_i = \\frac1\\nu  - 1$$ \n",
    " \n",
    " And for $$ \\sum_{i=1}^l C_i = \\nu \\sum_{i=1}^l \\beta_i \\leq \\nu (\\frac1\\nu -1) = 1 - \\nu$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.2 (2 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that for some $x_i$ it holds that $\\|\\phi(x_i) - h \\|^2 > R$.\n",
    "Such points are called **outliers**.\n",
    "\n",
    "* What are the values of $\\alpha_i$ and $\\beta_i$ for these points?\n",
    "* Argue that the sum of weights $C_i$ for the **outliers** is upper bounded by $\\nu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " * from $\\|\\phi(x_i) - h \\|^2 \\to  \\xi_i > 0 $, from complementary slackness condition$ \\beta_i = 0 \\to \\alpha_i = \\frac1\\nu C_i$\n",
    " \n",
    " * $$ \\sum_{i=1}^l C_i =  \\nu \\sum_{i=1}^l \\alpha_i \\leq \\nu $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.3: Very small $\\nu$ (1 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $\\nu < C_i$ for all $i=1,\\,\\ldots,\\,l$. Show that\n",
    "there are **no outliers** in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's look at all points: $l \\nu < \\sum_{i=1}^l C_i$ and if we have outliers here than for some points should be $\\sum_j C_j \\leq \\nu$ but there we have condradiction: $\\sum_j C_j \\leq \\nu < \\sum_{i=1}^l C_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.4 (1 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $C_i = \\tfrac1l$. Please, describe how $\\nu$ is related to the\n",
    "outliers in the datset, given the analysis above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for Case 7.3 $\\nu < C_i \\to \\nu < \\frac1 l$ no outliers\n",
    "$$\\sum_{j=1}^k C_j = \\frac{k}{l} \\leq \\nu \\to \\nu \\geq \\frac{k}{l}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
